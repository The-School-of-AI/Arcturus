{
  "directed": true,
  "multigraph": false,
  "graph": {
    "session_id": "auto_9a9168b6_20260124_113000",
    "original_query": "Find our the latest innovations, research, new releases from DeekSeek Related to LLMs, and save that in DeepSeek_Update.md",
    "file_manifest": [],
    "created_at": "2026-01-24T06:00:00.068391",
    "status": "completed",
    "globals_schema": {
      "deepseek_llm_innovations_research_T001": {
        "content": [
          {
            "type": "text",
            "text": [
              {
                "url": "https://deepseek.ai/blog/deepseek-engram-v4-architecture",
                "content": "Deepseek.ai is an independent website and is not affiliated with, sponsored by, or endorsed by Hangzhou DeepSeek Artificial Intelligence Co., Ltd.Machine Learning & Artificial Intelligence DeepSeek AI Home Products Compare Resources Pricing Discover more Paper paper ML Math Mathematics coding DeepSeek V3 AI Retrieval Information retrieval Home Blog DeepSeek Engram V4 Architecture Back to Blog Create AI Agents Discover more Paper paper Programming Search Engines Reference Machine Learning & Artificial Intelligence math Textiles & Nonwovens Office Supplies Math AI Research Breaking News ~10 min read January 12, 2026 DeepSeek Engram: V4 Architecture Revealed? Solving Transformer's Fatal Memory Flaw A groundbreaking paper titled \"Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models,\" signed by Liang Wenfeng, proposes the Engram module a new conditional memory architecture that provides O(1) knowledge lookup, potentially fixing Transformer's fundamental memory limitations and hinting at DeepSeek V4's architecture. Discover more Paper paper DeepSeek V3 Artificial intelligence Mathematics Retrieval Computer ai Math hardware Key Metrics at a Glance 27B Parameters outperforms baselineTextiles & Nonwovens 20-25% Optimal memory allocation <3% Overhead for 100B offloading 2.9k+ GitHub stars Engram Module New conditional memory layer for O(1) knowledge lookup U-Shaped Scaling Law 20-25% memory allocation optimal for performance 27B Parameters Outperforms MoE baseline under equal conditions <3% Overhead 100B parameters offloadable to CPU/SSD Reasoning Boost Frees early layers for deeper reasoning 2.9k+ GitHub Stars Strong community reception and interest Discover more Paper paper Programming Math Reference Office Supplies Computer Hardware Search Engines Machine Learning & Artificial Intelligence Chatbots & Virtual Assistants Table of Contents 1. The Problem: Transformer's Memory Limitation 2. Enter Engram: The 'Electronic Brain' Module 3. Core Architecture Deep Dive 4. The Optimal Memory Allocation (20-25%) 5. The U-Shaped Scaling Law Discovery 6. Why Reasoning and Math Improve 7. Long-Context Benefits 8. Hardware Cost Implications 9. Cross-Language Generalization 10. Could This Be DeepSeek V4? 11. GitHub Repository & Resources 12. Frequently Asked Questions Quick Summary DeepSeek, in collaboration with Peking University, has released a 33-page paper introducing Engram a new sparse memory module that complements Mixture of Experts (MoE). While MoE solves \"how to calculate less,\" Engram solves \"don't calculate blindly\" by providing deterministic O(1) knowledge lookup for static patterns like entity names and fixed phrases. The paper discovers a Sparsity Allocation Law: 20-25% of sparse parameters should go to memory, with the rest to computation. The Problem: Transformer's Memory Limitation Currently, Mixture of Experts (MoE) has become the mainstream architecture for large language models, powering systems like DeepSeek V3, Mixtral, and others. However, it's fundamentally still based on the Transformer architecture, which lacks a native \"knowledge lookup\" mechanism.Programming This means that many tasks that should be solved in O(1) time like retrieving factual information have to be \"simulated\" through extensive computations. The model essentially has to compute its way to remember things that could simply be looked up. Discover more Paper paper Information retrieval Math Mathematics Computer programming Computer DeepSeek R1 AI ML  Example: To identify the entity \"Diana, Princess of Wales,\" an LLM has to consume multi-layer attention and FFN to gradually combine features. In theory, this process could be completed through a single knowledge lookup operation like looking up a word in a dictionary. The paper argues that this is fundamentally wasteful: using expensive compute cycles to repeatedly \"rediscover\" static knowledge that could be retrieved in constant time. Enter Eng",
                "images": [],
                "rank": 1
              },
              {
                "url": "https://huggingface.co/collections/Presidentlin/deepseek-papers",
                "content": "Deepseek Papers - a Presidentlin Collection Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Presidentlin 's Collections ByteDance Papers Deepseek Papers AI Release Year Thread 2026 AI Release Year Thread 2025 AI Release Week Thread (12 January 2026) AI Release Week Thread (05 January 2026) AI Release Week Thread (29 December 2025) AI Release Week Thread (22 December 2025) AI Release Week Thread (15 December 2025) AI Release Week Thread (8 December 2025) AI Release Week Thread (1 December 2025) AI Release Week Thread (24 November 2025) AI Release Week Thread (10 November 2025) AI Release Week Thread (3 November 2025) AI Release Week Thread (27 October 2025) AI Release Week Thread (20 October 2025) AI Release Week Thread (13 October 2025) AI Release Week Thread (6 October 2025) AI Release Week Thread (29 September 2025) AI Release Week Thread (22 September 2025) AI Release Week Thread (15 September 2025) AI Release Week Thread (8 September 2025) AI Release Week Thread (1 September 2025) AI Release Week Thread (25 August 2025) AI Release Week Thread (18 August 2025) AI Release Week Thread (11 August 2025) AI Release Week Thread (04 August 2025) AI Release Week Thread (28 July 2025) AI Release Week Thread (21 July 2025) AI Release Week Thread (14 July 2025) AI Release Week Thread (7 July 2025) AI Release Week Thread (30 June 2025) AI Release Week Thread (23 June 2025) AI Release Week Thread (16 June 2025) AI Release Week Thread (2 June 2025) AI Release Week Thread (26 May 2025) AI Release Week Thread (19 May 2025) AI Release Week Thread (12 May 2025) Deepseek Papers updated 5 days ago Deepseek papers collection Upvote 313 +303 DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion  Prior Paper 2310.16818 Published Oct 25, 2023 33 DeepSeek LLM: Scaling Open-Source Language Models with Longtermism Paper 2401.02954 Published Jan 5, 2024 51 DeepSeekMoE: Towards Ultimate Expert Specialization in  Mixture-of-Experts Language Models Paper 2401.06066 Published Jan 11, 2024 58 DeepSeek-Coder: When the Large Language Model Meets Programming -- The  Rise of Code Intelligence Paper 2401.14196 Published Jan 25, 2024 69 DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open  Language Models Paper 2402.03300 Published Feb 5, 2024 139 DeepSeek-VL: Towards Real-World Vision-Language Understanding Paper 2403.05525 Published Mar 8, 2024 48 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts  Language Model Paper 2405.04434 Published May 7, 2024 24 DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale  Synthetic Data Paper 2405.14333 Published May 23, 2024 42 DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code  Intelligence Paper 2406.11931 Published Jun 17, 2024 67 Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for  Sparse Architectural Large Language Models Paper 2407.01906 Published Jul 2, 2024 45 DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for  Reinforcement Learning and Monte-Carlo Tree Search Paper 2408.08152 Published Aug 15, 2024 60 Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep  Learning Paper 2408.14158 Published Aug 26, 2024 3 Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts Paper 2408.15664 Published Aug 28, 2024 15 Janus: Decoupling Visual Encoding for Unified Multimodal Understanding  and Generation Paper 2410.13848 Published Oct 17, 2024 34 JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified  Multimodal Understanding and Generation Paper 2411.07975 Published Nov 12, 2024 31 DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced  Multimodal Understanding Paper 2412.10302 Published Dec 13, 2024 21 DeepSeek-V3 Technical Report Paper 2412.19437 Published Dec 27, 2024 74 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via  Reinforcement Learning Pape",
                "images": [],
                "rank": 2
              },
              {
                "url": "https://arxiv.org/pdf/2507.09955",
                "content": "[no text]",
                "images": [],
                "rank": 3
              },
              {
                "url": "https://wheremachinesthink.substack.com/p/deepseek-drops-yet-another-architectural",
                "content": "DeepSeek Drops Yet Another Architectural Innovation, Opening A New Front for Scaling Up LLMs WHERE MACHINES THINK Subscribe Sign in DeepSeek Drops Yet Another Architectural Innovation, Opening A New Front for Scaling Up LLMs DeepSeek soups up Transformers with Engram: A learnable, static memory that decouples knowledge storage and retrieval in LLMs from intense computation, freeing up the LLM layers for reasoning tasks Anil Ananthaswamy Jan 16, 2026 38 5 5 Share Some large language models can memorize entire books and regurgitate them almost verbatim. In one study, researchers successfully prompted an LLM to spit out, for example, a nearly complete Harry Potter and the Sorcerers Stone 1 . While this raises huge concerns about large-scale copyright violations, it's also true that we simultaneously want these models to remember facts about the world and not make them up, or hallucinate. And modern LLMs do ingest and remember an enormous amount of facts and patterns that they gleaned from the training data. But if we go looking for exactly where this knowledge is stored inside the model, we'll come up empty handed. That's because factual knowledge inside LLMs is not stored in some lookup table and fetched on demand. Rather, such knowledge is encoded in the weights or parameters of the model (the strengths of the connections between the artificial neurons that make up the networks); the LLM learns the values for these parameters during training. So each time we prompt an LLM, it has to use its parameters to reconstruct the relevant factual knowledge, making it a compute-intensive process, which then takes resources away from the reasoning needed to solve some task. DeepSeek write in their latest paper 2 : Language modeling entails two qualitatively different sub-tasks: compositional reasoning and knowledge retrieval. While the former demands deep, dynamic computation, a substantial portion of textsuch as named entities and formulaic patternsis local, static, and highly stereotyped Current LLMs are forced to simulate retrieval through computation [Italics mine]. What if one could separate the retrieval from most of the neural network's computations, turning the task into some computationally inexpensive lookup, and thus free up the models parameters for important tasks, such as logic and reasoning? More importantly, the storage and retrieval of knowledge (such as facts and common linguistic patterns) should depend on the statistics of the training data, and the process should be amenable to learning via the standard training algorithm, i.e. gradient descent done using backpropagation memory should not be hacked on as an afterthought. In other words, it should be part of an LLM's internal architecture. DeepSeek's paper suggests they have achieved just that, at least in part. Subscribe to receive new posts and support WHERE MACHINES THINK. Thank you! Subscribe They argue that their method creates a new primitive that can become an indispensable ingredient for LLMs. By leveraging decades-old ideas about n- grams from the field of natural language processing (NLP), DeepSeek's architectural tweak stores factual information in an embedding matrix (the values of this matrix are learned during training), and retrieves it on demand during inference using trivially little compute (relative to the compute needed for other tasks such as reasoning). The approach frees up the early layers of an LLM from having to reconstruct factual knowledge, effectively deepening the network for complex reasoning. Understanding DeepSeek's solution requires us to first take a brief look at n- grams, get a high-level overview of a modern Transformer-based LLM, and then delve into DeepSeek's innovation. Going Back to N-Grams DeepSeek designed a new, modular neural network block that can be inserted between Transformer layers of a large language model ( for those unfamiliar with a modern LLM, there's an overview of the architecture in the next section ). This ne",
                "images": [],
                "rank": 4
              },
              {
                "url": "https://introl.com/blog/deepseek-engram-conditional-memory-architecture-january-2026",
                "content": "DeepSeek's Engram Separates Memory from Reasoning in LLM Architecture Breakthrough | Introl Blog Services GPU Infrastructure Install, cable, test, commission Remote Hands 4-hour SLA support Data Center Migration Zero-downtime relocations Structured Cabling Fiber & containment Projects About Us Blog Careers Contact EN EN English ES Espaol DE Deutsch FR Franais ZH AR JA UK KO NL Nederlands ID Bahasa Indonesia PT Portugus HI VI Ting Vit TH Back EN English ES Espaol DE Deutsch FR Franais ZH AR JA UK KO NL Nederlands ID Bahasa Indonesia PT Portugus HI VI Ting Vit TH Blog DeepSeek's Engram Separates Memory from Reasoning in LLM Architecture Breakthrough DeepSeek publishes Engram, a conditional memory system that offloads static knowledge to DRAM while preserving GPU resources for complex reasoningachieving O(1) lookups and 97% long-context accuracy. Blake Crosley Jan 14, 2026 9 min read Disclaimer DeepSeek's Engram Separates Memory from Reasoning in LLM Architecture Breakthrough DeepSeek published research on January 12, 2026 introducing Engram, a conditional memory module that achieves constant-time knowledge retrieval by decoupling static pattern storage from dynamic reasoningpotentially bypassing the GPU high-bandwidth memory constraints that limit model scaling. TL;DR DeepSeek's Engram module introduces a new axis of sparsity for large language models, separating factual knowledge storage from computational reasoning. Testing on a 27-billion-parameter model shows benchmark improvements of 3-5 points across knowledge, reasoning, and coding tasks, with Needle-in-a-Haystack accuracy jumping from 84.2% to 97%. The researchers demonstrated offloading a 100-billion-parameter embedding table to system DRAM with throughput penalties below 3%. Co-authored by founder Liang Wenfeng and Peking University researchers, the fully open-source paper and code suggest Engram will underpin DeepSeek's forthcoming V4 model. The Problem: GPU Memory as Scaling Bottleneck Modern Transformers suffer from a fundamental architectural inefficiency. When an LLM needs to recall a static fact\"Paris is the capital of France\" or \"Diana, Princess of Wales\"it cannot query a database [1]. Instead, the model simulates retrieval through expensive computation, consuming multiple layers of attention and feed-forward networks to reconstruct patterns that could be handled by simple lookup [1]. Architecture Type Knowledge Handling GPU HBM Usage Standard Transformer All layers compute everything High Mixture-of-Experts (MoE) Conditional computation High Engram + MoE Static lookup + dynamic reasoning Optimized This inefficiency compounds as models scale. GPU high-bandwidth memory remains expensive and scarce [2]. The mere presence of larger, faster HBM does not yield sustained slackdevelopers immediately build larger models to fill it, ensuring memory remains the next bottleneck [3]. Engram Architecture: How It Works Engram introduces conditional memory as a complementary axis of sparsity, modernizing classic N-gram embeddings to perform constant-time O(1) lookups alongside the neural backbone [4]. Three Core Innovations Tokenizer Compression : Engram compresses equivalent tokens (different capitalizations of the same word) to canonical forms, reducing vocabulary size by 23% [5]. The compression pipeline converts tokens through text normalization (NFKC NFD strip accents lowercase whitespace collapse) and maps all tokens normalizing to the same string to a single compressed ID [6]. Multi-Head Hashing : Storing a table for every possible N-gram combination is intractable. Engram employs K distinct hash heads for each N-gram order to mitigate collisions [7]. When retrieving embeddings, the system aggregates across all heads, reducing the impact of any single collision [8]. Context-Aware Gating : Retrieved embeddings pass through a gating mechanism where the model's current hidden state acts as a query [9]. If retrieved memory contradicts the broader context,",
                "images": [],
                "rank": 5
              }
            ],
            "annotations": null,
            "_meta": null
          }
        ]
      },
      "deepseek_llm_new_releases_T002": {
        "content": [
          {
            "type": "text",
            "text": "[error] No search results found",
            "annotations": null,
            "_meta": null
          }
        ]
      },
      "combined_deepseek_llm_data_T003": {
        "innovations": [
          {
            "name": "Engram Module",
            "description": "A new conditional memory architecture designed to address Transformer's fundamental memory limitations. It provides O(1) knowledge lookup for static patterns by decoupling knowledge storage and retrieval from intensive computation. This frees up LLM layers for deeper reasoning.",
            "key_features": [
              "O(1) knowledge lookup",
              "Conditional memory",
              "New axis of sparsity",
              "Complements Mixture of Experts (MoE)",
              "Tokenizer Compression (23% vocabulary reduction)",
              "Multi-Head Hashing",
              "Context-Aware Gating"
            ],
            "benefits": [
              "Fixes Transformer's memory flaws",
              "Reduces compute waste for static knowledge retrieval",
              "Boosts reasoning capabilities",
              "Enables scaling of LLMs",
              "Reduces GPU memory bottleneck",
              "Potential for improved long-context accuracy (97% demonstrated)",
              "Offloads 100B parameters to CPU/SSD with <3% overhead"
            ],
            "associated_paper": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
            "release_date_mentioned": "January 12, 2026",
            "affiliation": "DeepSeek AI (collaboration with Peking University)"
          }
        ],
        "models_and_research": [
          {
            "name": "DeepSeek V3",
            "type": "LLM",
            "key_aspects": "Powered by Mixture of Experts (MoE) architecture."
          },
          {
            "name": "DeepSeek Engram V4 Architecture",
            "type": "Potential Architecture",
            "key_aspects": "Hinted to be underpinned by the Engram module."
          },
          {
            "name": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
            "type": "Research Paper",
            "publication_date": "January 5, 2024"
          },
          {
            "name": "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models",
            "type": "Research Paper",
            "publication_date": "January 11, 2024"
          },
          {
            "name": "DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence",
            "type": "LLM / Research Paper",
            "publication_date": "January 25, 2024"
          },
          {
            "name": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
            "type": "LLM / Research Paper",
            "publication_date": "February 5, 2024"
          },
          {
            "name": "DeepSeek-VL: Towards Real-World Vision-Language Understanding",
            "type": "LLM / Research Paper",
            "publication_date": "March 8, 2024"
          },
          {
            "name": "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model",
            "type": "LLM / Research Paper",
            "publication_date": "May 7, 2024"
          },
          {
            "name": "DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data",
            "type": "LLM / Research Paper",
            "publication_date": "May 23, 2024"
          },
          {
            "name": "DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence",
            "type": "LLM / Research Paper",
            "publication_date": "June 17, 2024"
          },
          {
            "name": "Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models",
            "type": "Research Paper",
            "publication_date": "July 2, 2024"
          },
          {
            "name": "DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search",
            "type": "LLM / Research Paper",
            "publication_date": "August 15, 2024"
          },
          {
            "name": "Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep Learning",
            "type": "Research Paper",
            "publication_date": "August 26, 2024"
          },
          {
            "name": "Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts",
            "type": "Research Paper",
            "publication_date": "August 28, 2024"
          },
          {
            "name": "Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation",
            "type": "Research Paper",
            "publication_date": "October 17, 2024"
          },
          {
            "name": "JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation",
            "type": "Research Paper",
            "publication_date": "November 12, 2024"
          },
          {
            "name": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding",
            "type": "LLM / Research Paper",
            "publication_date": "December 13, 2024"
          },
          {
            "name": "DeepSeek-V3 Technical Report",
            "type": "Research Paper",
            "publication_date": "December 27, 2024"
          },
          {
            "name": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
            "type": "LLM / Research Paper",
            "publication_date": null
          }
        ],
        "key_themes": [
          {
            "theme": "Architectural Innovations for LLMs",
            "details": "DeepSeek is actively researching and releasing novel architectures to overcome fundamental limitations in LLMs. The Engram module represents a significant step in separating knowledge retrieval from computational reasoning, offering O(1) lookup and reducing reliance on dense computation for static information. This directly addresses Transformer's memory inefficiency and GPU memory bottlenecks."
          },
          {
            "theme": "Advancements in Mixture-of-Experts (MoE)",
            "details": "MoE remains a core focus, with research exploring expert specialization (DeepSeekMoE, Let the Expert Stick to His Last), and efficient load balancing (Auxiliary-Loss-Free Load Balancing Strategy). DeepSeek-V2 and DeepSeek-VL2 are examples of MoE models."
          },
          {
            "theme": "Specialized LLMs for Code and Mathematics",
            "details": "DeepSeek is developing and releasing models tailored for specific domains, such as DeepSeek-Coder (and its V2 iteration) for code intelligence and DeepSeekMath for mathematical reasoning. They also focus on theorem proving with DeepSeek-Prover."
          },
          {
            "theme": "Multimodal Understanding and Generation",
            "details": "Research extends to vision-language capabilities with DeepSeek-VL and DeepSeek-VL2, and unified multimodal processing through models like Janus and JanusFlow."
          },
          {
            "theme": "Performance and Efficiency",
            "details": "A recurring theme is improving model performance (benchmarks, accuracy) while maintaining efficiency (economical, cost-effective, reduced overhead). Engram's ability to offload to CPU/SSD and optimize memory allocation contributes to this."
          },
          {
            "theme": "Open-Source Contributions",
            "details": "DeepSeek emphasizes open-source contributions, with papers and code for various models and research areas being made available, fostering community engagement (e.g., GitHub stars for Engram)."
          }
        ],
        "gap_analysis": [
          {
            "area": "New Releases (T002 Input)",
            "observation": "The input for 'deepseek_llm_new_releases_T002' indicated an error and no search results were found.",
            "implication": "No specific new *product* releases or announcements beyond research papers could be identified from this source. The combination relies heavily on the 'innovations_research' input for understanding DeepSeek's forward-looking developments."
          },
          {
            "area": "Engram V4 Specifics",
            "observation": "While Engram is presented as a key architectural innovation potentially underpinning DeepSeek V4, concrete details or a formal release announcement for 'DeepSeek V4' itself are not explicitly detailed in the provided texts. The association is speculative ('Could This Be DeepSeek V4?').",
            "implication": "The exact launch or specifications of DeepSeek V4 remain to be confirmed, though Engram is positioned as a foundational element."
          }
        ],
        "comparison_notes": [
          "The 'deepseek_llm_innovations_research_T001' provides in-depth technical details on the Engram module, its architecture, benefits, and the underlying research paper, while also listing a comprehensive set of DeepSeek's past research publications and model releases.",
          "The 'deepseek_llm_new_releases_T002' was effectively empty, meaning no new product releases were identified from that source. Therefore, the primary source of information for combining and structuring remains 'deepseek_llm_innovations_research_T001'.",
          "Engram is presented as a significant architectural innovation that directly addresses limitations found in existing Transformer and MoE architectures, suggesting it's a key enabler for future DeepSeek models, potentially including 'DeepSeek V4'."
        ]
      },
      "formatted_deepseek_update_T004": [],
      "file_DeepSeek_Update.md_T005": []
    }
  },
  "nodes": [
    {
      "description": "Initial Query",
      "agent": "System",
      "status": "completed",
      "output": null,
      "error": null,
      "cost": 0.0,
      "start_time": null,
      "end_time": null,
      "execution_time": 0.0,
      "id": "ROOT"
    },
    {
      "id": "Query",
      "description": "Formulate execution plan",
      "agent": "PlannerAgent",
      "status": "completed",
      "reads": [
        "original_query"
      ],
      "writes": [
        "plan_graph"
      ],
      "output": {
        "plan_graph": {
          "nodes": [
            {
              "id": "T001",
              "description": "Research latest innovations and research from DeepSeek related to LLMs.",
              "agent": "RetrieverAgent",
              "agent_prompt": "Search for the latest innovations, research papers, and announcements from DeepSeek specifically related to Large Language Models (LLMs). Focus on information published recently. Return a structured list of findings, including titles, brief summaries, and links if available.",
              "reads": [],
              "writes": [
                "deepseek_llm_innovations_research_T001"
              ]
            },
            {
              "id": "T002",
              "description": "Identify new releases and product updates from DeepSeek concerning LLMs.",
              "agent": "RetrieverAgent",
              "agent_prompt": "Search for new releases, product updates, and significant announcements from DeepSeek that are directly related to their LLM offerings. Focus on recent developments and key features. Return a structured list of releases, including product names, release dates, and key highlights.",
              "reads": [],
              "writes": [
                "deepseek_llm_new_releases_T002"
              ]
            },
            {
              "id": "T003",
              "description": "Synthesize and combine all gathered information about DeepSeek's LLM activities.",
              "agent": "ThinkerAgent",
              "agent_prompt": "Combine the information from 'deepseek_llm_innovations_research_T001' and 'deepseek_llm_new_releases_T002'. Identify overlapping themes, consolidate similar findings, and structure the data logically for a comprehensive update. Ensure all unique and relevant pieces of information are retained.",
              "reads": [
                "deepseek_llm_innovations_research_T001",
                "deepseek_llm_new_releases_T002"
              ],
              "writes": [
                "combined_deepseek_llm_data_T003"
              ]
            },
            {
              "id": "T004",
              "description": "Format the synthesized DeepSeek LLM update into a Markdown file.",
              "agent": "FormatterAgent",
              "agent_prompt": "Take the consolidated LLM information from 'combined_deepseek_llm_data_T003' and format it into a detailed, comprehensive Markdown report. Organize the content clearly with headings for innovations, research, and new releases. Ensure the output is suitable for saving as 'DeepSeek_Update.md'. Aim for a detailed and well-structured report, not a concise summary.",
              "reads": [
                "combined_deepseek_llm_data_T003"
              ],
              "writes": [
                "formatted_deepseek_update_T004"
              ]
            },
            {
              "id": "T005",
              "description": "Save the formatted DeepSeek LLM update to DeepSeek_Update.md.",
              "agent": "CoderAgent",
              "agent_prompt": "Save the content from 'formatted_deepseek_update_T004' into a file named 'DeepSeek_Update.md' in the DATA_DIR.",
              "reads": [
                "formatted_deepseek_update_T004"
              ],
              "writes": [
                "file_DeepSeek_Update.md_T005"
              ]
            }
          ],
          "edges": [
            {
              "source": "ROOT",
              "target": "T001"
            },
            {
              "source": "ROOT",
              "target": "T002"
            },
            {
              "source": "T001",
              "target": "T003"
            },
            {
              "source": "T002",
              "target": "T003"
            },
            {
              "source": "T003",
              "target": "T004"
            },
            {
              "source": "T004",
              "target": "T005"
            }
          ]
        },
        "next_step_id": "T001",
        "interpretation_confidence": 0.95,
        "ambiguity_notes": [],
        "cost": 0.00019380000000000002,
        "input_tokens": 126,
        "output_tokens": 453,
        "total_tokens": 579,
        "executed_model": "gemini:gemini-2.5-flash-lite"
      },
      "error": null,
      "cost": 0.0,
      "start_time": null,
      "end_time": "2026-01-24T06:00:03.577502",
      "execution_time": 0.0
    },
    {
      "id": "T001",
      "description": "Research latest innovations and research from DeepSeek related to LLMs.",
      "agent": "RetrieverAgent",
      "agent_prompt": "Search for the latest innovations, research papers, and announcements from DeepSeek specifically related to Large Language Models (LLMs). Focus on information published recently. Return a structured list of findings, including titles, brief summaries, and links if available.",
      "reads": [],
      "writes": [
        "deepseek_llm_innovations_research_T001"
      ],
      "status": "completed",
      "output": {
        "deepseek_llm_innovations_research_T001": [],
        "call_self": false,
        "code_variants": {
          "CODE_1A": "results = json.loads(search_web_with_text_content('DeepSeek LLM innovations research papers announcements latest', 10))\nreturn {'deepseek_llm_innovations_research_T001': results}",
          "CODE_1B": "urls = json.loads(fetch_search_urls('DeepSeek AI latest LLM developments research', 10))\nreturn {'deepseek_llm_innovations_research_T001': urls}"
        },
        "cost": 3.73e-05,
        "input_tokens": 193,
        "output_tokens": 45,
        "total_tokens": 238,
        "executed_model": "gemini:gemini-2.5-flash-lite",
        "execution_result": {
          "deepseek_llm_innovations_research_T001": {
            "content": [
              {
                "type": "text",
                "text": [
                  {
                    "url": "https://deepseek.ai/blog/deepseek-engram-v4-architecture",
                    "content": "Deepseek.ai is an independent website and is not affiliated with, sponsored by, or endorsed by Hangzhou DeepSeek Artificial Intelligence Co., Ltd.Machine Learning & Artificial Intelligence DeepSeek AI Home Products Compare Resources Pricing Discover more Paper paper ML Math Mathematics coding DeepSeek V3 AI Retrieval Information retrieval Home Blog DeepSeek Engram V4 Architecture Back to Blog Create AI Agents Discover more Paper paper Programming Search Engines Reference Machine Learning & Artificial Intelligence math Textiles & Nonwovens Office Supplies Math AI Research Breaking News ~10 min read January 12, 2026 DeepSeek Engram: V4 Architecture Revealed? Solving Transformer's Fatal Memory Flaw A groundbreaking paper titled \"Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models,\" signed by Liang Wenfeng, proposes the Engram module a new conditional memory architecture that provides O(1) knowledge lookup, potentially fixing Transformer's fundamental memory limitations and hinting at DeepSeek V4's architecture. Discover more Paper paper DeepSeek V3 Artificial intelligence Mathematics Retrieval Computer ai Math hardware Key Metrics at a Glance 27B Parameters outperforms baselineTextiles & Nonwovens 20-25% Optimal memory allocation <3% Overhead for 100B offloading 2.9k+ GitHub stars Engram Module New conditional memory layer for O(1) knowledge lookup U-Shaped Scaling Law 20-25% memory allocation optimal for performance 27B Parameters Outperforms MoE baseline under equal conditions <3% Overhead 100B parameters offloadable to CPU/SSD Reasoning Boost Frees early layers for deeper reasoning 2.9k+ GitHub Stars Strong community reception and interest Discover more Paper paper Programming Math Reference Office Supplies Computer Hardware Search Engines Machine Learning & Artificial Intelligence Chatbots & Virtual Assistants Table of Contents 1. The Problem: Transformer's Memory Limitation 2. Enter Engram: The 'Electronic Brain' Module 3. Core Architecture Deep Dive 4. The Optimal Memory Allocation (20-25%) 5. The U-Shaped Scaling Law Discovery 6. Why Reasoning and Math Improve 7. Long-Context Benefits 8. Hardware Cost Implications 9. Cross-Language Generalization 10. Could This Be DeepSeek V4? 11. GitHub Repository & Resources 12. Frequently Asked Questions Quick Summary DeepSeek, in collaboration with Peking University, has released a 33-page paper introducing Engram a new sparse memory module that complements Mixture of Experts (MoE). While MoE solves \"how to calculate less,\" Engram solves \"don't calculate blindly\" by providing deterministic O(1) knowledge lookup for static patterns like entity names and fixed phrases. The paper discovers a Sparsity Allocation Law: 20-25% of sparse parameters should go to memory, with the rest to computation. The Problem: Transformer's Memory Limitation Currently, Mixture of Experts (MoE) has become the mainstream architecture for large language models, powering systems like DeepSeek V3, Mixtral, and others. However, it's fundamentally still based on the Transformer architecture, which lacks a native \"knowledge lookup\" mechanism.Programming This means that many tasks that should be solved in O(1) time like retrieving factual information have to be \"simulated\" through extensive computations. The model essentially has to compute its way to remember things that could simply be looked up. Discover more Paper paper Information retrieval Math Mathematics Computer programming Computer DeepSeek R1 AI ML  Example: To identify the entity \"Diana, Princess of Wales,\" an LLM has to consume multi-layer attention and FFN to gradually combine features. In theory, this process could be completed through a single knowledge lookup operation like looking up a word in a dictionary. The paper argues that this is fundamentally wasteful: using expensive compute cycles to repeatedly \"rediscover\" static knowledge that could be retrieved in constant time. Enter Eng",
                    "images": [],
                    "rank": 1
                  },
                  {
                    "url": "https://huggingface.co/collections/Presidentlin/deepseek-papers",
                    "content": "Deepseek Papers - a Presidentlin Collection Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Presidentlin 's Collections ByteDance Papers Deepseek Papers AI Release Year Thread 2026 AI Release Year Thread 2025 AI Release Week Thread (12 January 2026) AI Release Week Thread (05 January 2026) AI Release Week Thread (29 December 2025) AI Release Week Thread (22 December 2025) AI Release Week Thread (15 December 2025) AI Release Week Thread (8 December 2025) AI Release Week Thread (1 December 2025) AI Release Week Thread (24 November 2025) AI Release Week Thread (10 November 2025) AI Release Week Thread (3 November 2025) AI Release Week Thread (27 October 2025) AI Release Week Thread (20 October 2025) AI Release Week Thread (13 October 2025) AI Release Week Thread (6 October 2025) AI Release Week Thread (29 September 2025) AI Release Week Thread (22 September 2025) AI Release Week Thread (15 September 2025) AI Release Week Thread (8 September 2025) AI Release Week Thread (1 September 2025) AI Release Week Thread (25 August 2025) AI Release Week Thread (18 August 2025) AI Release Week Thread (11 August 2025) AI Release Week Thread (04 August 2025) AI Release Week Thread (28 July 2025) AI Release Week Thread (21 July 2025) AI Release Week Thread (14 July 2025) AI Release Week Thread (7 July 2025) AI Release Week Thread (30 June 2025) AI Release Week Thread (23 June 2025) AI Release Week Thread (16 June 2025) AI Release Week Thread (2 June 2025) AI Release Week Thread (26 May 2025) AI Release Week Thread (19 May 2025) AI Release Week Thread (12 May 2025) Deepseek Papers updated 5 days ago Deepseek papers collection Upvote 313 +303 DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion  Prior Paper 2310.16818 Published Oct 25, 2023 33 DeepSeek LLM: Scaling Open-Source Language Models with Longtermism Paper 2401.02954 Published Jan 5, 2024 51 DeepSeekMoE: Towards Ultimate Expert Specialization in  Mixture-of-Experts Language Models Paper 2401.06066 Published Jan 11, 2024 58 DeepSeek-Coder: When the Large Language Model Meets Programming -- The  Rise of Code Intelligence Paper 2401.14196 Published Jan 25, 2024 69 DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open  Language Models Paper 2402.03300 Published Feb 5, 2024 139 DeepSeek-VL: Towards Real-World Vision-Language Understanding Paper 2403.05525 Published Mar 8, 2024 48 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts  Language Model Paper 2405.04434 Published May 7, 2024 24 DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale  Synthetic Data Paper 2405.14333 Published May 23, 2024 42 DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code  Intelligence Paper 2406.11931 Published Jun 17, 2024 67 Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for  Sparse Architectural Large Language Models Paper 2407.01906 Published Jul 2, 2024 45 DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for  Reinforcement Learning and Monte-Carlo Tree Search Paper 2408.08152 Published Aug 15, 2024 60 Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep  Learning Paper 2408.14158 Published Aug 26, 2024 3 Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts Paper 2408.15664 Published Aug 28, 2024 15 Janus: Decoupling Visual Encoding for Unified Multimodal Understanding  and Generation Paper 2410.13848 Published Oct 17, 2024 34 JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified  Multimodal Understanding and Generation Paper 2411.07975 Published Nov 12, 2024 31 DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced  Multimodal Understanding Paper 2412.10302 Published Dec 13, 2024 21 DeepSeek-V3 Technical Report Paper 2412.19437 Published Dec 27, 2024 74 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via  Reinforcement Learning Pape",
                    "images": [],
                    "rank": 2
                  },
                  {
                    "url": "https://arxiv.org/pdf/2507.09955",
                    "content": "[no text]",
                    "images": [],
                    "rank": 3
                  },
                  {
                    "url": "https://wheremachinesthink.substack.com/p/deepseek-drops-yet-another-architectural",
                    "content": "DeepSeek Drops Yet Another Architectural Innovation, Opening A New Front for Scaling Up LLMs WHERE MACHINES THINK Subscribe Sign in DeepSeek Drops Yet Another Architectural Innovation, Opening A New Front for Scaling Up LLMs DeepSeek soups up Transformers with Engram: A learnable, static memory that decouples knowledge storage and retrieval in LLMs from intense computation, freeing up the LLM layers for reasoning tasks Anil Ananthaswamy Jan 16, 2026 38 5 5 Share Some large language models can memorize entire books and regurgitate them almost verbatim. In one study, researchers successfully prompted an LLM to spit out, for example, a nearly complete Harry Potter and the Sorcerers Stone 1 . While this raises huge concerns about large-scale copyright violations, it's also true that we simultaneously want these models to remember facts about the world and not make them up, or hallucinate. And modern LLMs do ingest and remember an enormous amount of facts and patterns that they gleaned from the training data. But if we go looking for exactly where this knowledge is stored inside the model, we'll come up empty handed. That's because factual knowledge inside LLMs is not stored in some lookup table and fetched on demand. Rather, such knowledge is encoded in the weights or parameters of the model (the strengths of the connections between the artificial neurons that make up the networks); the LLM learns the values for these parameters during training. So each time we prompt an LLM, it has to use its parameters to reconstruct the relevant factual knowledge, making it a compute-intensive process, which then takes resources away from the reasoning needed to solve some task. DeepSeek write in their latest paper 2 : Language modeling entails two qualitatively different sub-tasks: compositional reasoning and knowledge retrieval. While the former demands deep, dynamic computation, a substantial portion of textsuch as named entities and formulaic patternsis local, static, and highly stereotyped Current LLMs are forced to simulate retrieval through computation [Italics mine]. What if one could separate the retrieval from most of the neural network's computations, turning the task into some computationally inexpensive lookup, and thus free up the models parameters for important tasks, such as logic and reasoning? More importantly, the storage and retrieval of knowledge (such as facts and common linguistic patterns) should depend on the statistics of the training data, and the process should be amenable to learning via the standard training algorithm, i.e. gradient descent done using backpropagation memory should not be hacked on as an afterthought. In other words, it should be part of an LLM's internal architecture. DeepSeek's paper suggests they have achieved just that, at least in part. Subscribe to receive new posts and support WHERE MACHINES THINK. Thank you! Subscribe They argue that their method creates a new primitive that can become an indispensable ingredient for LLMs. By leveraging decades-old ideas about n- grams from the field of natural language processing (NLP), DeepSeek's architectural tweak stores factual information in an embedding matrix (the values of this matrix are learned during training), and retrieves it on demand during inference using trivially little compute (relative to the compute needed for other tasks such as reasoning). The approach frees up the early layers of an LLM from having to reconstruct factual knowledge, effectively deepening the network for complex reasoning. Understanding DeepSeek's solution requires us to first take a brief look at n- grams, get a high-level overview of a modern Transformer-based LLM, and then delve into DeepSeek's innovation. Going Back to N-Grams DeepSeek designed a new, modular neural network block that can be inserted between Transformer layers of a large language model ( for those unfamiliar with a modern LLM, there's an overview of the architecture in the next section ). This ne",
                    "images": [],
                    "rank": 4
                  },
                  {
                    "url": "https://introl.com/blog/deepseek-engram-conditional-memory-architecture-january-2026",
                    "content": "DeepSeek's Engram Separates Memory from Reasoning in LLM Architecture Breakthrough | Introl Blog Services GPU Infrastructure Install, cable, test, commission Remote Hands 4-hour SLA support Data Center Migration Zero-downtime relocations Structured Cabling Fiber & containment Projects About Us Blog Careers Contact EN EN English ES Espaol DE Deutsch FR Franais ZH AR JA UK KO NL Nederlands ID Bahasa Indonesia PT Portugus HI VI Ting Vit TH Back EN English ES Espaol DE Deutsch FR Franais ZH AR JA UK KO NL Nederlands ID Bahasa Indonesia PT Portugus HI VI Ting Vit TH Blog DeepSeek's Engram Separates Memory from Reasoning in LLM Architecture Breakthrough DeepSeek publishes Engram, a conditional memory system that offloads static knowledge to DRAM while preserving GPU resources for complex reasoningachieving O(1) lookups and 97% long-context accuracy. Blake Crosley Jan 14, 2026 9 min read Disclaimer DeepSeek's Engram Separates Memory from Reasoning in LLM Architecture Breakthrough DeepSeek published research on January 12, 2026 introducing Engram, a conditional memory module that achieves constant-time knowledge retrieval by decoupling static pattern storage from dynamic reasoningpotentially bypassing the GPU high-bandwidth memory constraints that limit model scaling. TL;DR DeepSeek's Engram module introduces a new axis of sparsity for large language models, separating factual knowledge storage from computational reasoning. Testing on a 27-billion-parameter model shows benchmark improvements of 3-5 points across knowledge, reasoning, and coding tasks, with Needle-in-a-Haystack accuracy jumping from 84.2% to 97%. The researchers demonstrated offloading a 100-billion-parameter embedding table to system DRAM with throughput penalties below 3%. Co-authored by founder Liang Wenfeng and Peking University researchers, the fully open-source paper and code suggest Engram will underpin DeepSeek's forthcoming V4 model. The Problem: GPU Memory as Scaling Bottleneck Modern Transformers suffer from a fundamental architectural inefficiency. When an LLM needs to recall a static fact\"Paris is the capital of France\" or \"Diana, Princess of Wales\"it cannot query a database [1]. Instead, the model simulates retrieval through expensive computation, consuming multiple layers of attention and feed-forward networks to reconstruct patterns that could be handled by simple lookup [1]. Architecture Type Knowledge Handling GPU HBM Usage Standard Transformer All layers compute everything High Mixture-of-Experts (MoE) Conditional computation High Engram + MoE Static lookup + dynamic reasoning Optimized This inefficiency compounds as models scale. GPU high-bandwidth memory remains expensive and scarce [2]. The mere presence of larger, faster HBM does not yield sustained slackdevelopers immediately build larger models to fill it, ensuring memory remains the next bottleneck [3]. Engram Architecture: How It Works Engram introduces conditional memory as a complementary axis of sparsity, modernizing classic N-gram embeddings to perform constant-time O(1) lookups alongside the neural backbone [4]. Three Core Innovations Tokenizer Compression : Engram compresses equivalent tokens (different capitalizations of the same word) to canonical forms, reducing vocabulary size by 23% [5]. The compression pipeline converts tokens through text normalization (NFKC NFD strip accents lowercase whitespace collapse) and maps all tokens normalizing to the same string to a single compressed ID [6]. Multi-Head Hashing : Storing a table for every possible N-gram combination is intractable. Engram employs K distinct hash heads for each N-gram order to mitigate collisions [7]. When retrieving embeddings, the system aggregates across all heads, reducing the impact of any single collision [8]. Context-Aware Gating : Retrieved embeddings pass through a gating mechanism where the model's current hidden state acts as a query [9]. If retrieved memory contradicts the broader context,",
                    "images": [],
                    "rank": 5
                  }
                ],
                "annotations": null,
                "_meta": null
              }
            ]
          }
        },
        "execution_status": "success",
        "execution_error": null,
        "execution_time": "2026-01-24 11:30:24",
        "executed_variant": "CODE_1A",
        "execution_logs": ""
      },
      "error": null,
      "cost": 3.73e-05,
      "start_time": "2026-01-24T06:00:03.584031",
      "end_time": "2026-01-24T06:00:34.741004",
      "execution_time": 31.156973,
      "iterations": [
        {
          "iteration": 1,
          "output": {
            "deepseek_llm_innovations_research_T001": [],
            "call_self": false,
            "code_variants": {
              "CODE_1A": "results = json.loads(search_web_with_text_content('DeepSeek LLM innovations research papers announcements latest', 10))\nreturn {'deepseek_llm_innovations_research_T001': results}",
              "CODE_1B": "urls = json.loads(fetch_search_urls('DeepSeek AI latest LLM developments research', 10))\nreturn {'deepseek_llm_innovations_research_T001': urls}"
            },
            "cost": 3.73e-05,
            "input_tokens": 193,
            "output_tokens": 45,
            "total_tokens": 238,
            "executed_model": "gemini:gemini-2.5-flash-lite"
          },
          "execution_result": {
            "status": "success",
            "result": {
              "deepseek_llm_innovations_research_T001": {
                "content": [
                  {
                    "type": "text",
                    "text": [
                      {
                        "url": "https://huggingface.co/collections/Presidentlin/deepseek-papers",
                        "content": "Deepseek Papers - a Presidentlin Collection Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Presidentlin 's Collections ByteDance Papers Deepseek Papers AI Release Year Thread 2026 AI Release Year Thread 2025 AI Release Week Thread (12 January 2026) AI Release Week Thread (05 January 2026) AI Release Week Thread (29 December 2025) AI Release Week Thread (22 December 2025) AI Release Week Thread (15 December 2025) AI Release Week Thread (8 December 2025) AI Release Week Thread (1 December 2025) AI Release Week Thread (24 November 2025) AI Release Week Thread (10 November 2025) AI Release Week Thread (3 November 2025) AI Release Week Thread (27 October 2025) AI Release Week Thread (20 October 2025) AI Release Week Thread (13 October 2025) AI Release Week Thread (6 October 2025) AI Release Week Thread (29 September 2025) AI Release Week Thread (22 September 2025) AI Release Week Thread (15 September 2025) AI Release Week Thread (8 September 2025) AI Release Week Thread (1 September 2025) AI Release Week Thread (25 August 2025) AI Release Week Thread (18 August 2025) AI Release Week Thread (11 August 2025) AI Release Week Thread (04 August 2025) AI Release Week Thread (28 July 2025) AI Release Week Thread (21 July 2025) AI Release Week Thread (14 July 2025) AI Release Week Thread (7 July 2025) AI Release Week Thread (30 June 2025) AI Release Week Thread (23 June 2025) AI Release Week Thread (16 June 2025) AI Release Week Thread (2 June 2025) AI Release Week Thread (26 May 2025) AI Release Week Thread (19 May 2025) AI Release Week Thread (12 May 2025) Deepseek Papers updated 5 days ago Deepseek papers collection Upvote 313 +303 DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion  Prior Paper 2310.16818 Published Oct 25, 2023 33 DeepSeek LLM: Scaling Open-Source Language Models with Longtermism Paper 2401.02954 Published Jan 5, 2024 51 DeepSeekMoE: Towards Ultimate Expert Specialization in  Mixture-of-Experts Language Models Paper 2401.06066 Published Jan 11, 2024 58 DeepSeek-Coder: When the Large Language Model Meets Programming -- The  Rise of Code Intelligence Paper 2401.14196 Published Jan 25, 2024 69 DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open  Language Models Paper 2402.03300 Published Feb 5, 2024 139 DeepSeek-VL: Towards Real-World Vision-Language Understanding Paper 2403.05525 Published Mar 8, 2024 48 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts  Language Model Paper 2405.04434 Published May 7, 2024 24 DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale  Synthetic Data Paper 2405.14333 Published May 23, 2024 42 DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code  Intelligence Paper 2406.11931 Published Jun 17, 2024 67 Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for  Sparse Architectural Large Language Models Paper 2407.01906 Published Jul 2, 2024 45 DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for  Reinforcement Learning and Monte-Carlo Tree Search Paper 2408.08152 Published Aug 15, 2024 60 Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep  Learning Paper 2408.14158 Published Aug 26, 2024 3 Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts Paper 2408.15664 Published Aug 28, 2024 15 Janus: Decoupling Visual Encoding for Unified Multimodal Understanding  and Generation Paper 2410.13848 Published Oct 17, 2024 34 JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified  Multimodal Understanding and Generation Paper 2411.07975 Published Nov 12, 2024 31 DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced  Multimodal Understanding Paper 2412.10302 Published Dec 13, 2024 21 DeepSeek-V3 Technical Report Paper 2412.19437 Published Dec 27, 2024 74 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via  Reinforcement Learning Pape",
                        "images": [],
                        "rank": 1
                      },
                      {
                        "url": "https://arxiv.org/abs/2507.09955",
                        "content": "[2507.09955] DeepSeek: Paradigm Shifts and Technical Evolution in Large AI Models Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate > cs > arXiv:2507.09955 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About Computer Science > Artificial Intelligence arXiv:2507.09955 (cs) [Submitted on 14 Jul 2025] Title: DeepSeek: Paradigm Shifts and Technical Evolution in Large AI Models Authors: Luolin Xiong , Haofen Wang , Xi Chen , Lu Sheng , Yun Xiong , Jingping Liu , Yanghua Xiao , Huajun Chen , Qing-Long Han , Yang Tang View a PDF of the paper titled DeepSeek: Paradigm Shifts and Technical Evolution in Large AI Models, by Luolin Xiong and 9 other authors View PDF HTML (experimental) Abstract: DeepSeek, a Chinese Artificial Intelligence (AI) startup, has released their V3 and R1 series models, which attracted global attention due to their low cost, high performance, and open-source advantages. This paper begins by reviewing the evolution of large AI models focusing on paradigm shifts, the mainstream Large Language Model (LLM) paradigm, and the DeepSeek paradigm. Subsequently, the paper highlights novel algorithms introduced by DeepSeek, including Multi-head Latent Attention (MLA), Mixture-of-Experts (MoE), Multi-Token Prediction (MTP), and Group Relative Policy Optimization (GRPO). The paper then explores DeepSeek engineering breakthroughs in LLM scaling, training, inference, and system-level optimization architecture. Moreover, the impact of DeepSeek models on the competitive AI landscape is analyzed, comparing them to mainstream LLMs across various fields. Finally, the paper reflects on the insights gained from DeepSeek innovations and discusses future trends in the technical and engineering development of large AI models, particularly in data, training, and reasoning. Subjects: Artificial Intelligence (cs.AI) Cite as: arXiv:2507.09955 [cs.AI] (or arXiv:2507.09955v1 [cs.AI] for this version) https://doi.org/10.48550/arXiv.2507.09955 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Luolin Xiong [ view email ] [v1] Mon, 14 Jul 2025 06:10:30 UTC (2,987 KB) Full-text links: Access Paper: View a PDF of the paper titled DeepSeek: Paradigm Shifts and Technical Evolution in Large AI Models, by Luolin Xiong and 9 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.AI <prev | next> new | recent | 2025-07 Change to browse by: cs References & Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is",
                        "images": [],
                        "rank": 2
                      },
                      {
                        "url": "https://deepseek.ai/blog/deepseek-engram-v4-architecture",
                        "content": "Deepseek.ai is an independent website and is not affiliated with, sponsored by, or endorsed by Hangzhou DeepSeek Artificial Intelligence Co., Ltd. DeepSeek AI Home Products Compare Resources Pricing Discover more Paper paper Computer programming Computer DeepSeek V3 retrieval hardware ML AI Hardware Home Blog DeepSeek Engram V4 Architecture Back to Blog Create AI Agents Discover more paper Paper ML math Hardware Retrieval information retrieval hardware retrieval coding AI Research Breaking News ~10 min read January 12, 2026 DeepSeek Engram: V4 Architecture Revealed? Solving Transformer's Fatal Memory Flaw A groundbreaking paper titled \"Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models,\" signed by Liang Wenfeng, proposes the Engram module a new conditional memory architecture that provides O(1) knowledge lookup, potentially fixing Transformer's fundamental memory limitations and hinting at DeepSeek V4's architecture. Discover more Paper paper Hardware retrieval Artificial intelligence Computer hardware math AI hardware Mathematics Key Metrics at a Glance 27B Parameters outperforms baseline 20-25% Optimal memory allocation <3% Overhead for 100B offloading 2.9k+ GitHub stars Engram Module New conditional memory layer for O(1) knowledge lookup U-Shaped Scaling Law 20-25% memory allocation optimal for performance 27B Parameters Outperforms MoE baseline under equal conditions <3% Overhead 100B parameters offloadable to CPU/SSD Reasoning Boost Frees early layers for deeper reasoning 2.9k+ GitHub Stars Strong community reception and interest Table of Contents 1. The Problem: Transformer's Memory Limitation 2. Enter Engram: The 'Electronic Brain' Module 3. Core Architecture Deep Dive 4. The Optimal Memory Allocation (20-25%) 5. The U-Shaped Scaling Law Discovery 6. Why Reasoning and Math Improve 7. Long-Context Benefits 8. Hardware Cost Implications 9. Cross-Language Generalization 10. Could This Be DeepSeek V4? 11. GitHub Repository & Resources 12. Frequently Asked Questions Quick Summary DeepSeek, in collaboration with Peking University, has released a 33-page paper introducing Engram a new sparse memory module that complements Mixture of Experts (MoE). While MoE solves \"how to calculate less,\" Engram solves \"don't calculate blindly\" by providing deterministic O(1) knowledge lookup for static patterns like entity names and fixed phrases. The paper discovers a Sparsity Allocation Law: 20-25% of sparse parameters should go to memory, with the rest to computation. The Problem: Transformer's Memory Limitation Currently, Mixture of Experts (MoE) has become the mainstream architecture for large language models, powering systems like DeepSeek V3, Mixtral, and others. However, it's fundamentally still based on the Transformer architecture, which lacks a native \"knowledge lookup\" mechanism. This means that many tasks that should be solved in O(1) time like retrieving factual information have to be \"simulated\" through extensive computations. The model essentially has to compute its way to remember things that could simply be looked up. Discover more paper Paper DeepSeek R1 information retrieval Information retrieval DeepSeek V3 Deep Seek R1 Machine learning ai DeepSeek-R1  Example: To identify the entity \"Diana, Princess of Wales,\" an LLM has to consume multi-layer attention and FFN to gradually combine features. In theory, this process could be completed through a single knowledge lookup operation like looking up a word in a dictionary. The paper argues that this is fundamentally wasteful: using expensive compute cycles to repeatedly \"rediscover\" static knowledge that could be retrieved in constant time. Enter Engram: The \"Electronic Brain\" Module The term Engram originates from neurology, meaning \"memory trace\" an extensible and retrievable memory unit. DeepSeek's implementation modernizes the classic \"hashed N-gram embedding\" technique and transforms it into an extensibl",
                        "images": [],
                        "rank": 3
                      },
                      {
                        "url": "https://www.nature.com/articles/s41586-025-09422-z",
                        "content": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning | Nature Skip to main content Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain       the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in       Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles       and JavaScript. Advertisement View all journals Search Log in Content Explore content About the journal Publish with us Sign up for alerts RSS feed nature articles article DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning Download PDF Download PDF Article Open access Published: 17 September 2025 DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning Daya Guo 1 , Dejian Yang 1 , Haowei Zhang 1 , Junxiao Song 1 , Peiyi Wang 1 , Qihao Zhu 1 , Runxin Xu 1 , Ruoyu Zhang 1 , Shirong Ma 1 , Xiao Bi 1 , Xiaokang Zhang 1 , Xingkai Yu 1 , Yu Wu 1 , Z. F. Wu 1 , Zhibin Gou 1 , Zhihong Shao 1 , Zhuoshu Li 1 , Ziyi Gao 1 , Aixin Liu 1 , Bing Xue 1 , Bingxuan Wang 1 , Bochao Wu 1 , Bei Feng 1 , Chengda Lu 1 , Chenggang Zhao 1 , Chengqi Deng 1 , Chong Ruan 1 , Damai Dai 1 , Deli Chen 1 , Dongjie Ji 1 , Erhang Li 1 , Fangyun Lin 1 , Fucong Dai 1 , Fuli Luo 1 nAff2 , Guangbo Hao 1 , Guanting Chen 1 , Guowei Li 1 , H. Zhang 1 , Hanwei Xu 1 , Honghui Ding 1 , Huazuo Gao 1 , Hui Qu 1 , Hui Li 1 , Jianzhong Guo 1 , Jiashi Li 1 , Jingchang Chen 1 , Jingyang Yuan 1 , Jinhao Tu 1 nAff3 , Junjie Qiu 1 , Junlong Li 1 , J. L. Cai 1 , Jiaqi Ni 1 , Jian Liang 1 , Jin Chen 1 , Kai Dong 1 , Kai Hu 1 nAff4 , Kaichao You 1 , Kaige Gao 1 , Kang Guan 1 , Kexin Huang 1 nAff5 , Kuai Yu 1 , Lean Wang 1 , Lecong Zhang 1 , Liang Zhao 1 , Litong Wang 1 , Liyue Zhang 1 , Lei Xu 1 , Leyi Xia 1 , Mingchuan Zhang 1 , Minghua Zhang 1 , Minghui Tang 1 , Mingxu Zhou 1 , Meng Li 1 , Miaojun Wang 1 , Mingming Li 1 , Ning Tian 1 , Panpan Huang 1 , Peng Zhang 1 , Qiancheng Wang 1 , Qinyu Chen 1 , Qiushi Du 1 , Ruiqi Ge 1 , Ruisong Zhang 1 , Ruizhe Pan 1 , Runji Wang 1 , R. J. Chen 1 , R. L. Jin 1 , Ruyi Chen 1 , Shanghao Lu 1 , Shangyan Zhou 1 , Shanhuang Chen 1 , Shengfeng Ye 1 , Shiyu Wang 1 , Shuiping Yu 1 , Shunfeng Zhou 1 , Shuting Pan 1 , S. S. Li 1 , Shuang Zhou 1 , Shaoqing Wu 1 , Tao Yun 1 , Tian Pei 1 , Tianyu Sun 1 , T. Wang 1 , Wangding Zeng 1 , Wen Liu 1 , Wenfeng Liang 1 , Wenjun Gao 1 , Wenqin Yu ORCID: orcid.org/0000-0002-5715-3011 1 nAff5 , Wentao Zhang 1 , W. L. Xiao 1 , Wei An 1 , Xiaodong Liu 1 , Xiaohan Wang 1 , Xiaokang Chen 1 , Xiaotao Nie 1 , Xin Cheng 1 , Xin Liu 1 , Xin Xie 1 , Xingchao Liu 1 , Xinyu Yang 1 , Xinyuan Li 1 nAff5 , Xuecheng Su 1 , Xuheng Lin 1 , X. Q. Li 1 , Xiangyue Jin 1 , Xiaojin Shen 1 , Xiaosha Chen 1 , Xiaowen Sun 1 , Xiaoxiang Wang 1 , Xinnan Song 1 , Xinyi Zhou 1 , Xianzu Wang 1 , Xinxia Shan 1 , Y. K. Li 1 , Y. Q. Wang 1 , Y. X. Wei 1 , Yang Zhang 1 , Yanhong Xu 1 , Yao Li 1 , Yao Zhao 1 , Yaofeng Sun 1 , Yaohui Wang 1 , Yi Yu 1 , Yichao Zhang 1 , Yifan Shi 1 , Yiliang Xiong 1 , Ying He 1 , Yishi Piao 1 , Yisong Wang 1 , Yixuan Tan 1 , Yiyang Ma 1 , Yiyuan Liu 1 , Yongqiang Guo 1 , Yuan Ou 1 , Yuduan Wang 1 , Yue Gong 1 nAff5 , Yuheng Zou 1 , Yujia He 1 nAff5 , Yunfan Xiong 1 , Yuxiang Luo 1 , Yuxiang You 1 , Yuxuan Liu 1 , Yuyang Zhou 1 , Y. X. Zhu 1 , Yanping Huang 1 , Yaohui Li 1 , Yi Zheng 1 , Yuchen Zhu 1 , Yunxian Ma 1 , Ying Tang 1 , Yukun Zha 1 , Yuting Yan 1 , Z. Z. Ren 1 , Zehui Ren 1 , Zhangli Sha 1 , Zhe Fu 1 , Zhean Xu 1 , Zhenda Xie 1 , Zhengyan Zhang 1 , Zhewen Hao 1 , Zhicheng Ma 1 , Zhigang Yan 1 , Zhiyu Wu 1 , Zihui Gu 1 , Zijia Zhu 1 , Zijun Liu 1 nAff6 , Zilin Li 1 , Ziwei Xie 1 , Ziyang Song 1 nAff7 , Zizheng Pan 1 , Zhen Huang 1 , Zhipeng Xu 1 , Zhongyu Zhang 1 & Zhen Zhang 1 Show authors Nature volume 645 , pages 633638 ( 2025 ) Cite this article 337k Accesses 211 Citations 2478 Altmetric Metrics details Subjects",
                        "images": [],
                        "rank": 4
                      },
                      {
                        "url": "https://introl.com/blog/deepseek-engram-conditional-memory-architecture-january-2026",
                        "content": "DeepSeek's Engram Separates Memory from Reasoning in LLM Architecture Breakthrough | Introl Blog Services GPU Infrastructure Install, cable, test, commission Remote Hands 4-hour SLA support Data Center Migration Zero-downtime relocations Structured Cabling Fiber & containment Projects About Us Blog Careers Contact EN EN English ES Espaol DE Deutsch FR Franais ZH AR JA UK KO NL Nederlands ID Bahasa Indonesia PT Portugus HI VI Ting Vit TH Back EN English ES Espaol DE Deutsch FR Franais ZH AR JA UK KO NL Nederlands ID Bahasa Indonesia PT Portugus HI VI Ting Vit TH Blog DeepSeek's Engram Separates Memory from Reasoning in LLM Architecture Breakthrough DeepSeek publishes Engram, a conditional memory system that offloads static knowledge to DRAM while preserving GPU resources for complex reasoningachieving O(1) lookups and 97% long-context accuracy. Blake Crosley Jan 14, 2026 9 min read Disclaimer DeepSeek's Engram Separates Memory from Reasoning in LLM Architecture Breakthrough DeepSeek published research on January 12, 2026 introducing Engram, a conditional memory module that achieves constant-time knowledge retrieval by decoupling static pattern storage from dynamic reasoningpotentially bypassing the GPU high-bandwidth memory constraints that limit model scaling. TL;DR DeepSeek's Engram module introduces a new axis of sparsity for large language models, separating factual knowledge storage from computational reasoning. Testing on a 27-billion-parameter model shows benchmark improvements of 3-5 points across knowledge, reasoning, and coding tasks, with Needle-in-a-Haystack accuracy jumping from 84.2% to 97%. The researchers demonstrated offloading a 100-billion-parameter embedding table to system DRAM with throughput penalties below 3%. Co-authored by founder Liang Wenfeng and Peking University researchers, the fully open-source paper and code suggest Engram will underpin DeepSeek's forthcoming V4 model. The Problem: GPU Memory as Scaling Bottleneck Modern Transformers suffer from a fundamental architectural inefficiency. When an LLM needs to recall a static fact\"Paris is the capital of France\" or \"Diana, Princess of Wales\"it cannot query a database [1]. Instead, the model simulates retrieval through expensive computation, consuming multiple layers of attention and feed-forward networks to reconstruct patterns that could be handled by simple lookup [1]. Architecture Type Knowledge Handling GPU HBM Usage Standard Transformer All layers compute everything High Mixture-of-Experts (MoE) Conditional computation High Engram + MoE Static lookup + dynamic reasoning Optimized This inefficiency compounds as models scale. GPU high-bandwidth memory remains expensive and scarce [2]. The mere presence of larger, faster HBM does not yield sustained slackdevelopers immediately build larger models to fill it, ensuring memory remains the next bottleneck [3]. Engram Architecture: How It Works Engram introduces conditional memory as a complementary axis of sparsity, modernizing classic N-gram embeddings to perform constant-time O(1) lookups alongside the neural backbone [4]. Three Core Innovations Tokenizer Compression : Engram compresses equivalent tokens (different capitalizations of the same word) to canonical forms, reducing vocabulary size by 23% [5]. The compression pipeline converts tokens through text normalization (NFKC NFD strip accents lowercase whitespace collapse) and maps all tokens normalizing to the same string to a single compressed ID [6]. Multi-Head Hashing : Storing a table for every possible N-gram combination is intractable. Engram employs K distinct hash heads for each N-gram order to mitigate collisions [7]. When retrieving embeddings, the system aggregates across all heads, reducing the impact of any single collision [8]. Context-Aware Gating : Retrieved embeddings pass through a gating mechanism where the model's current hidden state acts as a query [9]. If retrieved memory contradicts the broader context,",
                        "images": [],
                        "rank": 5
                      }
                    ],
                    "annotations": null,
                    "_meta": null
                  }
                ]
              }
            },
            "raw": {
              "deepseek_llm_innovations_research_T001": {
                "content": [
                  {
                    "type": "text",
                    "text": [
                      {
                        "url": "https://huggingface.co/collections/Presidentlin/deepseek-papers",
                        "content": "Deepseek Papers - a Presidentlin Collection Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Presidentlin 's Collections ByteDance Papers Deepseek Papers AI Release Year Thread 2026 AI Release Year Thread 2025 AI Release Week Thread (12 January 2026) AI Release Week Thread (05 January 2026) AI Release Week Thread (29 December 2025) AI Release Week Thread (22 December 2025) AI Release Week Thread (15 December 2025) AI Release Week Thread (8 December 2025) AI Release Week Thread (1 December 2025) AI Release Week Thread (24 November 2025) AI Release Week Thread (10 November 2025) AI Release Week Thread (3 November 2025) AI Release Week Thread (27 October 2025) AI Release Week Thread (20 October 2025) AI Release Week Thread (13 October 2025) AI Release Week Thread (6 October 2025) AI Release Week Thread (29 September 2025) AI Release Week Thread (22 September 2025) AI Release Week Thread (15 September 2025) AI Release Week Thread (8 September 2025) AI Release Week Thread (1 September 2025) AI Release Week Thread (25 August 2025) AI Release Week Thread (18 August 2025) AI Release Week Thread (11 August 2025) AI Release Week Thread (04 August 2025) AI Release Week Thread (28 July 2025) AI Release Week Thread (21 July 2025) AI Release Week Thread (14 July 2025) AI Release Week Thread (7 July 2025) AI Release Week Thread (30 June 2025) AI Release Week Thread (23 June 2025) AI Release Week Thread (16 June 2025) AI Release Week Thread (2 June 2025) AI Release Week Thread (26 May 2025) AI Release Week Thread (19 May 2025) AI Release Week Thread (12 May 2025) Deepseek Papers updated 5 days ago Deepseek papers collection Upvote 313 +303 DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion  Prior Paper 2310.16818 Published Oct 25, 2023 33 DeepSeek LLM: Scaling Open-Source Language Models with Longtermism Paper 2401.02954 Published Jan 5, 2024 51 DeepSeekMoE: Towards Ultimate Expert Specialization in  Mixture-of-Experts Language Models Paper 2401.06066 Published Jan 11, 2024 58 DeepSeek-Coder: When the Large Language Model Meets Programming -- The  Rise of Code Intelligence Paper 2401.14196 Published Jan 25, 2024 69 DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open  Language Models Paper 2402.03300 Published Feb 5, 2024 139 DeepSeek-VL: Towards Real-World Vision-Language Understanding Paper 2403.05525 Published Mar 8, 2024 48 DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts  Language Model Paper 2405.04434 Published May 7, 2024 24 DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale  Synthetic Data Paper 2405.14333 Published May 23, 2024 42 DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code  Intelligence Paper 2406.11931 Published Jun 17, 2024 67 Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for  Sparse Architectural Large Language Models Paper 2407.01906 Published Jul 2, 2024 45 DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for  Reinforcement Learning and Monte-Carlo Tree Search Paper 2408.08152 Published Aug 15, 2024 60 Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep  Learning Paper 2408.14158 Published Aug 26, 2024 3 Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts Paper 2408.15664 Published Aug 28, 2024 15 Janus: Decoupling Visual Encoding for Unified Multimodal Understanding  and Generation Paper 2410.13848 Published Oct 17, 2024 34 JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified  Multimodal Understanding and Generation Paper 2411.07975 Published Nov 12, 2024 31 DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced  Multimodal Understanding Paper 2412.10302 Published Dec 13, 2024 21 DeepSeek-V3 Technical Report Paper 2412.19437 Published Dec 27, 2024 74 DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via  Reinforcement Learning Pape",
                        "images": [],
                        "rank": 1
                      },
                      {
                        "url": "https://arxiv.org/abs/2507.09955",
                        "content": "[2507.09955] DeepSeek: Paradigm Shifts and Technical Evolution in Large AI Models Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate > cs > arXiv:2507.09955 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About Computer Science > Artificial Intelligence arXiv:2507.09955 (cs) [Submitted on 14 Jul 2025] Title: DeepSeek: Paradigm Shifts and Technical Evolution in Large AI Models Authors: Luolin Xiong , Haofen Wang , Xi Chen , Lu Sheng , Yun Xiong , Jingping Liu , Yanghua Xiao , Huajun Chen , Qing-Long Han , Yang Tang View a PDF of the paper titled DeepSeek: Paradigm Shifts and Technical Evolution in Large AI Models, by Luolin Xiong and 9 other authors View PDF HTML (experimental) Abstract: DeepSeek, a Chinese Artificial Intelligence (AI) startup, has released their V3 and R1 series models, which attracted global attention due to their low cost, high performance, and open-source advantages. This paper begins by reviewing the evolution of large AI models focusing on paradigm shifts, the mainstream Large Language Model (LLM) paradigm, and the DeepSeek paradigm. Subsequently, the paper highlights novel algorithms introduced by DeepSeek, including Multi-head Latent Attention (MLA), Mixture-of-Experts (MoE), Multi-Token Prediction (MTP), and Group Relative Policy Optimization (GRPO). The paper then explores DeepSeek engineering breakthroughs in LLM scaling, training, inference, and system-level optimization architecture. Moreover, the impact of DeepSeek models on the competitive AI landscape is analyzed, comparing them to mainstream LLMs across various fields. Finally, the paper reflects on the insights gained from DeepSeek innovations and discusses future trends in the technical and engineering development of large AI models, particularly in data, training, and reasoning. Subjects: Artificial Intelligence (cs.AI) Cite as: arXiv:2507.09955 [cs.AI] (or arXiv:2507.09955v1 [cs.AI] for this version) https://doi.org/10.48550/arXiv.2507.09955 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Luolin Xiong [ view email ] [v1] Mon, 14 Jul 2025 06:10:30 UTC (2,987 KB) Full-text links: Access Paper: View a PDF of the paper titled DeepSeek: Paradigm Shifts and Technical Evolution in Large AI Models, by Luolin Xiong and 9 other authors View PDF HTML (experimental) TeX Source view license Current browse context: cs.AI <prev | next> new | recent | 2025-07 Change to browse by: cs References & Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is",
                        "images": [],
                        "rank": 2
                      },
                      {
                        "url": "https://deepseek.ai/blog/deepseek-engram-v4-architecture",
                        "content": "Deepseek.ai is an independent website and is not affiliated with, sponsored by, or endorsed by Hangzhou DeepSeek Artificial Intelligence Co., Ltd. DeepSeek AI Home Products Compare Resources Pricing Discover more Paper paper Computer programming Computer DeepSeek V3 retrieval hardware ML AI Hardware Home Blog DeepSeek Engram V4 Architecture Back to Blog Create AI Agents Discover more paper Paper ML math Hardware Retrieval information retrieval hardware retrieval coding AI Research Breaking News ~10 min read January 12, 2026 DeepSeek Engram: V4 Architecture Revealed? Solving Transformer's Fatal Memory Flaw A groundbreaking paper titled \"Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models,\" signed by Liang Wenfeng, proposes the Engram module a new conditional memory architecture that provides O(1) knowledge lookup, potentially fixing Transformer's fundamental memory limitations and hinting at DeepSeek V4's architecture. Discover more Paper paper Hardware retrieval Artificial intelligence Computer hardware math AI hardware Mathematics Key Metrics at a Glance 27B Parameters outperforms baseline 20-25% Optimal memory allocation <3% Overhead for 100B offloading 2.9k+ GitHub stars Engram Module New conditional memory layer for O(1) knowledge lookup U-Shaped Scaling Law 20-25% memory allocation optimal for performance 27B Parameters Outperforms MoE baseline under equal conditions <3% Overhead 100B parameters offloadable to CPU/SSD Reasoning Boost Frees early layers for deeper reasoning 2.9k+ GitHub Stars Strong community reception and interest Table of Contents 1. The Problem: Transformer's Memory Limitation 2. Enter Engram: The 'Electronic Brain' Module 3. Core Architecture Deep Dive 4. The Optimal Memory Allocation (20-25%) 5. The U-Shaped Scaling Law Discovery 6. Why Reasoning and Math Improve 7. Long-Context Benefits 8. Hardware Cost Implications 9. Cross-Language Generalization 10. Could This Be DeepSeek V4? 11. GitHub Repository & Resources 12. Frequently Asked Questions Quick Summary DeepSeek, in collaboration with Peking University, has released a 33-page paper introducing Engram a new sparse memory module that complements Mixture of Experts (MoE). While MoE solves \"how to calculate less,\" Engram solves \"don't calculate blindly\" by providing deterministic O(1) knowledge lookup for static patterns like entity names and fixed phrases. The paper discovers a Sparsity Allocation Law: 20-25% of sparse parameters should go to memory, with the rest to computation. The Problem: Transformer's Memory Limitation Currently, Mixture of Experts (MoE) has become the mainstream architecture for large language models, powering systems like DeepSeek V3, Mixtral, and others. However, it's fundamentally still based on the Transformer architecture, which lacks a native \"knowledge lookup\" mechanism. This means that many tasks that should be solved in O(1) time like retrieving factual information have to be \"simulated\" through extensive computations. The model essentially has to compute its way to remember things that could simply be looked up. Discover more paper Paper DeepSeek R1 information retrieval Information retrieval DeepSeek V3 Deep Seek R1 Machine learning ai DeepSeek-R1  Example: To identify the entity \"Diana, Princess of Wales,\" an LLM has to consume multi-layer attention and FFN to gradually combine features. In theory, this process could be completed through a single knowledge lookup operation like looking up a word in a dictionary. The paper argues that this is fundamentally wasteful: using expensive compute cycles to repeatedly \"rediscover\" static knowledge that could be retrieved in constant time. Enter Engram: The \"Electronic Brain\" Module The term Engram originates from neurology, meaning \"memory trace\" an extensible and retrievable memory unit. DeepSeek's implementation modernizes the classic \"hashed N-gram embedding\" technique and transforms it into an extensibl",
                        "images": [],
                        "rank": 3
                      },
                      {
                        "url": "https://www.nature.com/articles/s41586-025-09422-z",
                        "content": "DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning | Nature Skip to main content Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain       the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in       Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles       and JavaScript. Advertisement View all journals Search Log in Content Explore content About the journal Publish with us Sign up for alerts RSS feed nature articles article DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning Download PDF Download PDF Article Open access Published: 17 September 2025 DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning Daya Guo 1 , Dejian Yang 1 , Haowei Zhang 1 , Junxiao Song 1 , Peiyi Wang 1 , Qihao Zhu 1 , Runxin Xu 1 , Ruoyu Zhang 1 , Shirong Ma 1 , Xiao Bi 1 , Xiaokang Zhang 1 , Xingkai Yu 1 , Yu Wu 1 , Z. F. Wu 1 , Zhibin Gou 1 , Zhihong Shao 1 , Zhuoshu Li 1 , Ziyi Gao 1 , Aixin Liu 1 , Bing Xue 1 , Bingxuan Wang 1 , Bochao Wu 1 , Bei Feng 1 , Chengda Lu 1 , Chenggang Zhao 1 , Chengqi Deng 1 , Chong Ruan 1 , Damai Dai 1 , Deli Chen 1 , Dongjie Ji 1 , Erhang Li 1 , Fangyun Lin 1 , Fucong Dai 1 , Fuli Luo 1 nAff2 , Guangbo Hao 1 , Guanting Chen 1 , Guowei Li 1 , H. Zhang 1 , Hanwei Xu 1 , Honghui Ding 1 , Huazuo Gao 1 , Hui Qu 1 , Hui Li 1 , Jianzhong Guo 1 , Jiashi Li 1 , Jingchang Chen 1 , Jingyang Yuan 1 , Jinhao Tu 1 nAff3 , Junjie Qiu 1 , Junlong Li 1 , J. L. Cai 1 , Jiaqi Ni 1 , Jian Liang 1 , Jin Chen 1 , Kai Dong 1 , Kai Hu 1 nAff4 , Kaichao You 1 , Kaige Gao 1 , Kang Guan 1 , Kexin Huang 1 nAff5 , Kuai Yu 1 , Lean Wang 1 , Lecong Zhang 1 , Liang Zhao 1 , Litong Wang 1 , Liyue Zhang 1 , Lei Xu 1 , Leyi Xia 1 , Mingchuan Zhang 1 , Minghua Zhang 1 , Minghui Tang 1 , Mingxu Zhou 1 , Meng Li 1 , Miaojun Wang 1 , Mingming Li 1 , Ning Tian 1 , Panpan Huang 1 , Peng Zhang 1 , Qiancheng Wang 1 , Qinyu Chen 1 , Qiushi Du 1 , Ruiqi Ge 1 , Ruisong Zhang 1 , Ruizhe Pan 1 , Runji Wang 1 , R. J. Chen 1 , R. L. Jin 1 , Ruyi Chen 1 , Shanghao Lu 1 , Shangyan Zhou 1 , Shanhuang Chen 1 , Shengfeng Ye 1 , Shiyu Wang 1 , Shuiping Yu 1 , Shunfeng Zhou 1 , Shuting Pan 1 , S. S. Li 1 , Shuang Zhou 1 , Shaoqing Wu 1 , Tao Yun 1 , Tian Pei 1 , Tianyu Sun 1 , T. Wang 1 , Wangding Zeng 1 , Wen Liu 1 , Wenfeng Liang 1 , Wenjun Gao 1 , Wenqin Yu ORCID: orcid.org/0000-0002-5715-3011 1 nAff5 , Wentao Zhang 1 , W. L. Xiao 1 , Wei An 1 , Xiaodong Liu 1 , Xiaohan Wang 1 , Xiaokang Chen 1 , Xiaotao Nie 1 , Xin Cheng 1 , Xin Liu 1 , Xin Xie 1 , Xingchao Liu 1 , Xinyu Yang 1 , Xinyuan Li 1 nAff5 , Xuecheng Su 1 , Xuheng Lin 1 , X. Q. Li 1 , Xiangyue Jin 1 , Xiaojin Shen 1 , Xiaosha Chen 1 , Xiaowen Sun 1 , Xiaoxiang Wang 1 , Xinnan Song 1 , Xinyi Zhou 1 , Xianzu Wang 1 , Xinxia Shan 1 , Y. K. Li 1 , Y. Q. Wang 1 , Y. X. Wei 1 , Yang Zhang 1 , Yanhong Xu 1 , Yao Li 1 , Yao Zhao 1 , Yaofeng Sun 1 , Yaohui Wang 1 , Yi Yu 1 , Yichao Zhang 1 , Yifan Shi 1 , Yiliang Xiong 1 , Ying He 1 , Yishi Piao 1 , Yisong Wang 1 , Yixuan Tan 1 , Yiyang Ma 1 , Yiyuan Liu 1 , Yongqiang Guo 1 , Yuan Ou 1 , Yuduan Wang 1 , Yue Gong 1 nAff5 , Yuheng Zou 1 , Yujia He 1 nAff5 , Yunfan Xiong 1 , Yuxiang Luo 1 , Yuxiang You 1 , Yuxuan Liu 1 , Yuyang Zhou 1 , Y. X. Zhu 1 , Yanping Huang 1 , Yaohui Li 1 , Yi Zheng 1 , Yuchen Zhu 1 , Yunxian Ma 1 , Ying Tang 1 , Yukun Zha 1 , Yuting Yan 1 , Z. Z. Ren 1 , Zehui Ren 1 , Zhangli Sha 1 , Zhe Fu 1 , Zhean Xu 1 , Zhenda Xie 1 , Zhengyan Zhang 1 , Zhewen Hao 1 , Zhicheng Ma 1 , Zhigang Yan 1 , Zhiyu Wu 1 , Zihui Gu 1 , Zijia Zhu 1 , Zijun Liu 1 nAff6 , Zilin Li 1 , Ziwei Xie 1 , Ziyang Song 1 nAff7 , Zizheng Pan 1 , Zhen Huang 1 , Zhipeng Xu 1 , Zhongyu Zhang 1 & Zhen Zhang 1 Show authors Nature volume 645 , pages 633638 ( 2025 ) Cite this article 337k Accesses 211 Citations 2478 Altmetric Metrics details Subjects",
                        "images": [],
                        "rank": 4
                      },
                      {
                        "url": "https://introl.com/blog/deepseek-engram-conditional-memory-architecture-january-2026",
                        "content": "DeepSeek's Engram Separates Memory from Reasoning in LLM Architecture Breakthrough | Introl Blog Services GPU Infrastructure Install, cable, test, commission Remote Hands 4-hour SLA support Data Center Migration Zero-downtime relocations Structured Cabling Fiber & containment Projects About Us Blog Careers Contact EN EN English ES Espaol DE Deutsch FR Franais ZH AR JA UK KO NL Nederlands ID Bahasa Indonesia PT Portugus HI VI Ting Vit TH Back EN English ES Espaol DE Deutsch FR Franais ZH AR JA UK KO NL Nederlands ID Bahasa Indonesia PT Portugus HI VI Ting Vit TH Blog DeepSeek's Engram Separates Memory from Reasoning in LLM Architecture Breakthrough DeepSeek publishes Engram, a conditional memory system that offloads static knowledge to DRAM while preserving GPU resources for complex reasoningachieving O(1) lookups and 97% long-context accuracy. Blake Crosley Jan 14, 2026 9 min read Disclaimer DeepSeek's Engram Separates Memory from Reasoning in LLM Architecture Breakthrough DeepSeek published research on January 12, 2026 introducing Engram, a conditional memory module that achieves constant-time knowledge retrieval by decoupling static pattern storage from dynamic reasoningpotentially bypassing the GPU high-bandwidth memory constraints that limit model scaling. TL;DR DeepSeek's Engram module introduces a new axis of sparsity for large language models, separating factual knowledge storage from computational reasoning. Testing on a 27-billion-parameter model shows benchmark improvements of 3-5 points across knowledge, reasoning, and coding tasks, with Needle-in-a-Haystack accuracy jumping from 84.2% to 97%. The researchers demonstrated offloading a 100-billion-parameter embedding table to system DRAM with throughput penalties below 3%. Co-authored by founder Liang Wenfeng and Peking University researchers, the fully open-source paper and code suggest Engram will underpin DeepSeek's forthcoming V4 model. The Problem: GPU Memory as Scaling Bottleneck Modern Transformers suffer from a fundamental architectural inefficiency. When an LLM needs to recall a static fact\"Paris is the capital of France\" or \"Diana, Princess of Wales\"it cannot query a database [1]. Instead, the model simulates retrieval through expensive computation, consuming multiple layers of attention and feed-forward networks to reconstruct patterns that could be handled by simple lookup [1]. Architecture Type Knowledge Handling GPU HBM Usage Standard Transformer All layers compute everything High Mixture-of-Experts (MoE) Conditional computation High Engram + MoE Static lookup + dynamic reasoning Optimized This inefficiency compounds as models scale. GPU high-bandwidth memory remains expensive and scarce [2]. The mere presence of larger, faster HBM does not yield sustained slackdevelopers immediately build larger models to fill it, ensuring memory remains the next bottleneck [3]. Engram Architecture: How It Works Engram introduces conditional memory as a complementary axis of sparsity, modernizing classic N-gram embeddings to perform constant-time O(1) lookups alongside the neural backbone [4]. Three Core Innovations Tokenizer Compression : Engram compresses equivalent tokens (different capitalizations of the same word) to canonical forms, reducing vocabulary size by 23% [5]. The compression pipeline converts tokens through text normalization (NFKC NFD strip accents lowercase whitespace collapse) and maps all tokens normalizing to the same string to a single compressed ID [6]. Multi-Head Hashing : Storing a table for every possible N-gram combination is intractable. Engram employs K distinct hash heads for each N-gram order to mitigate collisions [7]. When retrieving embeddings, the system aggregates across all heads, reducing the impact of any single collision [8]. Context-Aware Gating : Retrieved embeddings pass through a gating mechanism where the model's current hidden state acts as a query [9]. If retrieved memory contradicts the broader context,",
                        "images": [],
                        "rank": 5
                      }
                    ],
                    "annotations": null,
                    "_meta": null
                  }
                ]
              }
            },
            "logs": "\n\u001b[1m  RetrieverAgent finished\u001b[0m\n{'output_keys': ['deepseek_llm_new_releases_T002',\n                 'call_self',\n                 'code_variants']}\n\n\u001b[1m \u001b[0m\u001b[1m[\u001b[0m\u001b[1mCODE:\u001b[0m\u001b[1m]\u001b[0m\u001b[1m: deepseek_llm_new_releases_T002 = \u001b[0m\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n\u001b[1mtotal_tokens = \u001b[0m\u001b[1;36m243\u001b[0m\n\u001b[1mexecuted_model = \u001b[0m\u001b[1;32m'gemini:gemini-2.5-flash-lite'\u001b[0m\n\u001b[1mreads_data = \u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n\u001b[1mresults = \u001b[0m\u001b[1;35mjson.loads\u001b[0m\u001b[1m(\u001b[0m\u001b[1;35msearch_web_with_text_content\u001b[0m\u001b[1m(\u001b[0m\u001b[1;32m'DeepSeek LLM new releases \u001b[0m\n\u001b[1;32mproduct updates announcements'\u001b[0m\u001b[1m, \u001b[0m\u001b[1;36m8\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n\u001b[1mreturn \u001b[0m\u001b[1m{\u001b[0m\u001b[1;32m'deepseek_llm_new_releases_T002'\u001b[0m\u001b[1m: results\u001b[0m\u001b[1m}\u001b[0m\n",
            "execution_time": "2026-01-24 11:30:10",
            "total_time": "13.077",
            "executed_variant": "CODE_1A"
          }
        }
      ],
      "input_tokens": 193,
      "output_tokens": 45,
      "total_tokens": 238
    },
    {
      "id": "T002",
      "description": "Identify new releases and product updates from DeepSeek concerning LLMs.",
      "agent": "RetrieverAgent",
      "agent_prompt": "Search for new releases, product updates, and significant announcements from DeepSeek that are directly related to their LLM offerings. Focus on recent developments and key features. Return a structured list of releases, including product names, release dates, and key highlights.",
      "reads": [],
      "writes": [
        "deepseek_llm_new_releases_T002"
      ],
      "status": "completed",
      "output": {
        "deepseek_llm_new_releases_T002": [],
        "call_self": false,
        "code_variants": {
          "CODE_1A": "results = json.loads(search_web_with_text_content('DeepSeek LLM new releases product updates announcements', 8))\nreturn {'deepseek_llm_new_releases_T002': results}",
          "CODE_1B": "results = json.loads(search_web_with_text_content('DeepSeek AI latest LLM research features', 6))\nreturn {'deepseek_llm_new_releases_T002': results}"
        },
        "cost": 3.78e-05,
        "input_tokens": 198,
        "output_tokens": 45,
        "total_tokens": 243,
        "executed_model": "gemini:gemini-2.5-flash-lite",
        "execution_result": {
          "deepseek_llm_new_releases_T002": {
            "content": [
              {
                "type": "text",
                "text": "[error] No search results found",
                "annotations": null,
                "_meta": null
              }
            ]
          }
        },
        "execution_status": "success",
        "execution_error": null,
        "execution_time": "2026-01-24 11:30:34",
        "executed_variant": "CODE_1A",
        "execution_logs": ""
      },
      "error": null,
      "cost": 3.78e-05,
      "start_time": "2026-01-24T06:00:03.584678",
      "end_time": "2026-01-24T06:01:37.361269",
      "execution_time": 93.776591,
      "iterations": [
        {
          "iteration": 1,
          "output": {
            "deepseek_llm_new_releases_T002": [],
            "call_self": false,
            "code_variants": {
              "CODE_1A": "results = json.loads(search_web_with_text_content('DeepSeek LLM new releases product updates announcements', 8))\nreturn {'deepseek_llm_new_releases_T002': results}",
              "CODE_1B": "results = json.loads(search_web_with_text_content('DeepSeek AI latest LLM research features', 6))\nreturn {'deepseek_llm_new_releases_T002': results}"
            },
            "cost": 3.78e-05,
            "input_tokens": 198,
            "output_tokens": 45,
            "total_tokens": 243,
            "executed_model": "gemini:gemini-2.5-flash-lite"
          },
          "execution_result": {
            "status": "success",
            "result": {
              "deepseek_llm_new_releases_T002": {
                "content": [
                  {
                    "type": "text",
                    "text": [
                      {
                        "url": "https://www.cnbc.com/2025/09/30/whats-new-in-deepseeks-latest-model-deepseek-v3point2-exp.html",
                        "content": "What's new in DeepSeek's latest model: DeepSeek-V3.2-Exp Skip Navigation Markets Pre-Markets U.S. Markets Europe Markets China Markets Asia Markets World Markets Currencies Cryptocurrency Futures & Commodities Bonds Funds & ETFs Business Economy Finance Health & Science Media Real Estate Energy Climate Transportation Investigations Industrials Retail Wealth Sports Life Small Business Investing Personal Finance Fintech Financial Advisors Options Action ETF Street Buffett Archive Earnings Trader Talk Tech Cybersecurity AI Enterprise Internet Media Mobile Social Media CNBC Disruptor 50 Tech Guide Politics White House Policy Defense Congress Expanding Opportunity Europe Politics China Politics Asia Politics World Politics Video Latest Video Full Episodes Livestream Top Video Live Audio Europe TV Asia TV CNBC Podcasts CEO Interviews Digital Originals Watchlist Investing Club Trust Portfolio Analysis Trade Alerts Meeting Videos Homestretch Jim's Columns Education Subscribe PRO Pro News Josh Brown Mike Santoli Calls of the Day My Portfolio Livestream Full Episodes Stock Screener Market Forecast Options Investing Chart Investing Subscribe Livestream Menu Make It select USA INTL Livestream Search quotes, news & videos Livestream Watchlist SIGN IN Create free account Markets Business Investing Tech Politics Video Watchlist Investing Club PRO Livestream Menu Tech China's DeepSeek launches next-gen AI model. Here's what makes it different Published Tue, Sep 30 2025 4:43 AM EDT Updated Tue, Sep 30 2025 5:16 AM EDT Tasmin Lockwood WATCH LIVE Key Points DeepSeek announced on Monday the release of an experimental version of its current model DeepSeek-V3.1-Terminus. Despite speculation of a bubble forming, AI remains at the centre of geopolitical competition with the U.S. and China vying for the winning spot. \"This is DeepSeek's value prop all over: efficiency is becoming as important as raw power,\" according to Nick Patience, VP and Practice Lead for AI at The Futurum Group. Anna Barclay | Getty Images News | Getty Images Chinese startup DeepSeek's latest experimental model promises to increase efficiency and improve AI's ability to handle a lot of information at a fraction of the cost, but questions remain over how effective and safe the architecture is. DeepSeek sent Silicon Valley into a frenzy when it launched its first model R1 out of nowhere last year, showing that it's possible to train large language models (LLMs) quickly, on less powerful chips, using fewer resources. The company released DeepSeek-V3.2-Exp on Monday, an experimental version of its current model DeepSeek-V3.1-Terminus, which builds further on its mission to increase efficiency in AI systems, according to a post on the AI forum Hugging Face . \"DeepSeek V3.2 continues the focus on efficiency, cost reduction, and open-source sharing,\" Adina Yakefu, Chinese community lead at Hugging Face, told CNBC. \"The big improvement is a new feature called DSA (DeepSeek Sparse Attention), which makes the AI better at handling long documents and conversations. It also cuts the cost of running the AI in half compared to the previous version.\" \"It's significant because it should make the model faster and more cost-effective to use without a noticeable drop in performance,\" said Nick Patience, vice president and practice lead for AI at The Futurum Group. \"This makes powerful AI more accessible to developers, researchers, and smaller companies, potentially leading to a wave of new and innovative applications.\" The pros and cons of sparse attention An AI model makes decisions based on its training data and new information, such as a prompt. Say an airline wants to find the best route from A to B, while there are many options, not all are feasible. By filtering out the less viable routes, you dramatically reduce the amount of time, fuel and, ultimately, money, needed to make the journey. That is exactly sparse attention does, it only factors in data that it thinks is important given the task",
                        "images": [],
                        "rank": 1
                      },
                      {
                        "url": "https://llm-stats.com/ai-news",
                        "content": "LLM News Today (January 2026) Open Source LLM Updates & AI Model Releases llm-stats .com Leaderboards Arenas Benchmarks Compare News Advertise Search K Toggle theme Join our Discord Discuss models, benchmarks & more Join the community Join Follow us on X Latest AI news and updates Follow for updates Follow Subscribe to our Newsletter Weekly AI updates delivered to your inbox Stay updated Subscribe Advertise with us Reach 200k+ engineers, researchers and founders 200k+ monthly users Learn more Kimi K2 Base GPQA 0.5 Llama 3.1 Nemotron Nano 8B V1 GPQA 0.5 Claude 3 Haiku GPQA 0.3 GPT-4o GPQA 0.5 DeepSeek-V3.1 GPQA 0.7 DeepSeek R1 Zero GPQA 0.7 Qwen3 VL 8B Thinking GPQA 0.7 Kimi K2-Thinking-0905 GPQA 0.8 Gemini 3 Flash GPQA 0.9 GLM-4.5 GPQA 0.8 Qwen2 7B Instruct GPQA 0.3 MiMo-V2-Flash GPQA 0.8 Llama 3.1 405B Instruct GPQA 0.5 DeepSeek R1 Distill Llama 8B GPQA 0.5 Qwen2.5 7B Instruct GPQA 0.4 Phi-3.5-mini-instruct GPQA 0.3 Qwen3 VL 4B Thinking GPQA 0.6 GPT-5 High GPQA 0.9 Grok-2 mini GPQA 0.5 o1 GPQA 0.8 Kimi K2 Base GPQA 0.5 Llama 3.1 Nemotron Nano 8B V1 GPQA 0.5 Claude 3 Haiku GPQA 0.3 GPT-4o GPQA 0.5 DeepSeek-V3.1 GPQA 0.7 DeepSeek R1 Zero GPQA 0.7 Qwen3 VL 8B Thinking GPQA 0.7 Kimi K2-Thinking-0905 GPQA 0.8 Gemini 3 Flash GPQA 0.9 GLM-4.5 GPQA 0.8 Qwen2 7B Instruct GPQA 0.3 MiMo-V2-Flash GPQA 0.8 Llama 3.1 405B Instruct GPQA 0.5 DeepSeek R1 Distill Llama 8B GPQA 0.5 Qwen2.5 7B Instruct GPQA 0.4 Phi-3.5-mini-instruct GPQA 0.3 Qwen3 VL 4B Thinking GPQA 0.6 GPT-5 High GPQA 0.9 Grok-2 mini GPQA 0.5 o1 GPQA 0.8 AI News AI Trends LLM Updates Saturday, January 24 LLM News Today Your daily source for LLM news, open source LLM updates, and large language model news. Breaking announcements, new AI model releases, LLM benchmark news, and the latest updates from the AI industry. LLM News New Releases Open Source Benchmarks Resources FAQ Today Reddit 3m ago One-Minute Daily AI News 1/23/2026 Meta is stopping teens from chatting with its AI characters.[1] GitHub Releases Copilot-SDK to Embed Its Agentic Runtime in Any App.[2] Intel struggles to meet AI data center demand, shares drop 13%.[ Read article TechCrunch 4m ago Legal AI giant Harvey acquires Hexus as competition heats up in legal tech Hexus founder and CEO Sakshi Pratap, who previously held engineering roles at Walmart, Oracle, and Google, tells TechCrunch that her San Francisco-based team has already joined Harvey, while the start Read article Reddit 37m ago AI makes humans useless People who say AI makes humans useless are missing the point. Real data from Anthropics AI usage research shows the opposite, that users with higher skills and more education tend to get way better r Read article Yesterday TechXplore 10h ago The next generation of disinformation: AI swarms can threaten democracy by manufacturing fake public consensus An international research team involving Konstanz scientist David Garcia warns that the next generation of influence operations may not look like obvious \"copy-paste bots,\" but like coordinated commun Read article Reddit 11h ago AMD Ryzen AI Software 1.7 released for improved performance on NPUs, new model support submitted by /u/Fcking_Chuck [link] [comments] Read article TechXplore 12h ago Stress-testing AI vision systems: Rethinking how adversarial images are generated Deep neural networks (DNNs) have become a cornerstone of modern AI technology, driving a thriving field of research in image-related tasks. These systems have found applications in medical diagnosis, Read article TechCrunch 12h ago Meta pauses teen access to AI characters ahead of new version Meta today said that it is pausing teens' access to its AI characters globally across all its apps. The company mentioned that it is not abandoning its efforts but wants to develop an updated version Read article TechCrunch 13h ago AI CEOs transformed Davos into a tech conference The World Economic Forums annual meeting in Davos felt different this",
                        "images": [],
                        "rank": 2
                      },
                      {
                        "url": "https://api-docs.deepseek.com/news/news251201",
                        "content": "DeepSeek-V3.2 Release | DeepSeek API Docs Skip to main content DeepSeek API Docs English English DeepSeek Platform Quick Start Your First API Call Models & Pricing The Temperature Parameter Token & Token Usage Rate Limit Error Codes News DeepSeek-V3.2 Release 2025/12/01 DeepSeek-V3.2-Exp Release 2025/09/29 DeepSeek V3.1 Update 2025/09/22 DeepSeek V3.1 Release 2025/08/21 DeepSeek-R1-0528 Release 2025/05/28 DeepSeek-V3-0324 Release 2025/03/25 DeepSeek-R1 Release 2025/01/20 DeepSeek APP 2025/01/15 Introducing DeepSeek-V3 2024/12/26 DeepSeek-V2.5-1210 Release 2024/12/10 DeepSeek-R1-Lite Release 2024/11/20 DeepSeek-V2.5 Release 2024/09/05 Context Caching is Available 2024/08/02 New API Features 2024/07/25 API Reference API Guides Thinking Mode Multi-round Conversation Chat Prefix Completion (Beta) FIM Completion (Beta) JSON Output Tool Calls Context Caching Anthropic API Other Resources Integrations API Status Page FAQ Change Log News DeepSeek-V3.2 Release 2025/12/01 DeepSeek-V3.2 Release Launching DeepSeek-V3.2 & DeepSeek-V3.2-Speciale Reasoning-first models built for agents! DeepSeek-V3.2: Official successor to V3.2-Exp. Now live on App, Web & API. DeepSeek-V3.2-Speciale: Pushing the boundaries of reasoning capabilities. API-only for now. Tech report: https://huggingface.co/deepseek-ai/DeepSeek-V3.2/resolve/main/assets/paper.pdf World-Leading Reasoning V3.2: Balanced inference vs. length. Your daily driver at GPT-5 level performance. V3.2-Speciale: Maxed-out reasoning capabilities. Rivals Gemini-3.0-Pro. Gold-Medal Performance: V3.2-Speciale attains gold-level results in IMO, CMO, ICPC World Finals & IOI 2025. Note: V3.2-Speciale dominates complex tasks but requires higher token usage. Currently API-only (no tool-use) to support community evaluation & research. Thinking in Tool-Use Introduces a new massive agent training data synthesis method covering 1,800+ environments & 85k+ complex instructions. DeepSeek-V3.2 is our first model to integrate thinking directly into tool-use, and also supports tool-use in both thinking and non-thinking modes. API Update V3.2: Same usage pattern as V3.2-Exp. V3.2-Speciale: Served via a temporary endpoint: base_url=\" https://api.deepseek.com/v3.2_speciale_expires_on_20251215 \". Same pricing as V3.2, no tool calls, available until Dec 15th, 2025, 15:59 (UTC Time). V3.2 now supports Thinking in Tool-Use details: https://api-docs.deepseek.com/guides/thinking_mode Open Source Release DeepSeek-V3.2 Model: https://huggingface.co/deepseek-ai/DeepSeek-V3.2 DeepSeek-V3.2-Speciale Model: https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale Tech report: https://huggingface.co/deepseek-ai/DeepSeek-V3.2/resolve/main/assets/paper.pdf Previous Error Codes Next Introducing DeepSeek-V3.2-Exp WeChat Official Account Community Email Discord Twitter More GitHub Copyright 2025 DeepSeek, Inc.",
                        "images": [],
                        "rank": 3
                      },
                      {
                        "url": "https://www.producthunt.com/products/deepseek/launches",
                        "content": "DeepSeek Latest Launches (2026) | Product Hunt Best Products Launches Launch archive Most-loved launches by the community Launch Guide Checklists and pro tips for launching News Newsletter The best of Product Hunt, every day Stories Tech news, interviews, and tips from makers Changelog New Product Hunt features and releases Forums Forums Ask questions, find support, and connect Streaks The most active community members Events Meet others online and in-person Advertise Subscribe Sign in DeepSeek Open-source LLM optimized for advanced reasoning and code 4.8 40 reviews 2.1K followers Open-source LLM optimized for advanced reasoning and code 4.8 40 reviews 2.1K followers Visit website LLMs AI Chatbots Foundation Models Intelligent assistant for coding, content creation, file reading, and more. Upload documents, engage in extended conversations, and receive expert assistance in AI, natural language processing, and beyond. Overview Launches 12 Reviews 40 Alternatives Customers Built with Forum More DeepSeek launches Launch date DeepSeek V3.2 Open-Source LLM matching GPT-5 Launched on December 2nd, 2025 6 55 DeepSeek-V3.2 Reasoning-first models built for agents Launched on December 2nd, 2025 2 205 DeepSeekMath-V2 IMO Gold level reasoning, fully open. Launched on November 28th, 2025 1 16 DeepSeek-OCR Read documents like an image Launched on October 21st, 2025 11 363 DeepSeek-V3.2-Exp Long-context efficiency with DeepSeek Sparse Attention Launched on September 30th, 2025 2 171 DeepSeek-V3.1-Terminus A refined agentic model for developers Launched on September 23rd, 2025 6 156 DeepSeek-V3.1 Our first step toward the agent era Launched on August 22nd, 2025 9 438 DeepSeek-R1-0528 New open-source LLM that rivals o3 in coding & reasoning Launched on May 29th, 2025 6 230 DeepSeek For Android DeepSeek's official AI assistant for free Launched on January 17th, 2025 1 36 DeepSeek for iOS Your Al assistant powered by DeepSeek-V3 Launched on January 15th, 2025 9 373 Follow DeepSeek Add to collection Share Company Info chat.deepseek.com/coder App Store DeepSeek Info Launched in 2024 View 12 launches Forum p/deepseek Awards View all Similar Products Claude by Anthropic A family of foundational AI models 5.0 ( 594 reviews ) LLMs AI Chatbots OpenAI APIs and tools for building AI products 5.0 ( 672 reviews ) LLMs AI Chatbots ChatGPT by OpenAI Get answers. Find inspiration. Be more productive. 4.8 ( 638 reviews ) LLMs AI Chatbots Gemini Google's answer to GPT-4 4.8 ( 120 reviews ) LLMs AI Chatbots Mistral AI Open and portable generative AI for devs and businesses 5.0 ( 32 reviews ) LLMs AI Chatbots View more Subscribe Sign in Social X Top Product Categories Engineering & Development LLMs Productivity Marketing & Sales Design & Creative Social & Community Finance AI Agents Engineering & Development Vibe Coding Tools AI Coding Agents AI Code Editors LLMs AI Chatbots AI Infrastructure Tools Prompt Engineering Tools Productivity AI notetakers Note and writing apps Team collaboration software Search Marketing & Sales Lead generation software Marketing automation platforms Design & Creative Video editing Design resources Graphic design tools AI Generative Media Social & Community Social Networking Professional networking platforms Community management Finance Accounting software Fundraising resources Investing AI Agents AI Voice Agents AI Agent Automation See All Categories >> Trending categories Top reviewed Trending products Top forum threads Trending categories Vibe Coding Tools AI Dictation Apps AI notetakers Code Review Tools No-code Platforms Figma Plugins Static site generators Top reviewed Lovable n8n Attio PostHog Vapi Granola Raycast Supabase Trending products Lovable Screen Studio bolt.new Wispr Flow Framer Replit Vapi Granola Top forum threads Cursor or Claude Code? POLL: Domain or product first? YC deadline in <2 weeks; Who's applying? We Got into YC, Got Kicked Out, and Fought Our Way Back How Wispr Flow found PMF through pivot Best Vibe",
                        "images": [],
                        "rank": 4
                      },
                      {
                        "url": "https://huggingface.co/deepseek-ai/DeepSeek-R1-0528",
                        "content": "deepseek-ai/DeepSeek-R1-0528 Hugging Face Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up deepseek-ai / DeepSeek-R1-0528 like 2.39k Follow DeepSeek 115k Text Generation Transformers Safetensors deepseek_v3 conversational custom_code text-generation-inference fp8 arxiv: 2501.12948 License: mit Model card Files Files and versions xet Community 113 Deploy Use this model DeepSeek-R1-0528 1. Introduction 2. Evaluation Results DeepSeek-R1-0528 DeepSeek-R1-0528-Qwen3-8B 3. Chat Website & API Platform 4. How to Run Locally System Prompt Temperature Prompts for File Uploading and Web Search 5. License 6. Citation 7. Contact DeepSeek-R1-0528 Paper Link 1. Introduction The DeepSeek R1 model has undergone a minor version upgrade, with the current version being DeepSeek-R1-0528. In the latest update, DeepSeek R1 has significantly improved its depth of reasoning and inference capabilities by leveraging increased computational resources and introducing algorithmic optimization mechanisms during post-training. The model has demonstrated outstanding performance across various benchmark evaluations, including mathematics, programming, and general logic. Its overall performance is now approaching that of leading models, such as O3 and Gemini 2.5 Pro. Compared to the previous version, the upgraded model shows significant improvements in handling complex reasoning tasks. For instance, in the AIME 2025 test, the models accuracy has increased from 70% in the previous version to 87.5% in the current version. This advancement stems from enhanced thinking depth during the reasoning process: in the AIME test set, the previous model used an average of 12K tokens per question, whereas the new version averages 23K tokens per question. Beyond its improved reasoning capabilities, this version also offers a reduced hallucination rate, enhanced support for function calling, and better experience for vibe coding. 2. Evaluation Results DeepSeek-R1-0528 For all our models, the maximum generation length is set to 64K tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 16 responses per query to estimate pass@1. Category Benchmark (Metric) DeepSeek R1 DeepSeek R1 0528 General MMLU-Redux (EM) 92.9 93.4 MMLU-Pro (EM) 84.0 85.0 GPQA-Diamond (Pass@1) 71.5 81.0 SimpleQA (Correct) 30.1 27.8 FRAMES (Acc.) 82.5 83.0 Humanity's Last Exam (Pass@1) 8.5 17.7 Code LiveCodeBench (2408-2505) (Pass@1) 63.5 73.3 Codeforces-Div1 (Rating) 1530 1930 SWE Verified (Resolved) 49.2 57.6 Aider-Polyglot (Acc.) 53.3 71.6 Math AIME 2024 (Pass@1) 79.8 91.4 AIME 2025 (Pass@1) 70.0 87.5 HMMT 2025 (Pass@1) 41.7 79.4 CNMO 2024 (Pass@1) 78.8 86.9 Tools BFCL_v3_MultiTurn (Acc) - 37.0 Tau-Bench  (Pass@1) - 53.5(Airline)/63.9(Retail) Note: We use Agentless framework to evaluate model performance on SWE-Verified. We only evaluate text-only prompts in HLE testsets. GPT-4.1 is employed to act user role in Tau-bench evaluation. DeepSeek-R1-0528-Qwen3-8B Meanwhile, we distilled the chain-of-thought from DeepSeek-R1-0528 to post-train Qwen3 8B Base, obtaining DeepSeek-R1-0528-Qwen3-8B. This model achieves state-of-the-art (SOTA) performance among open-source models on the AIME 2024, surpassing Qwen3 8B by +10.0% and matching the performance of Qwen3-235B-thinking. We believe that the chain-of-thought from DeepSeek-R1-0528 will hold significant importance for both academic research on reasoning models and industrial development focused on small-scale models. AIME 24 AIME 25 HMMT Feb 25 GPQA Diamond LiveCodeBench (2408-2505) Qwen3-235B-A22B 85.7 81.5 62.5 71.1 66.5 Qwen3-32B 81.4 72.9 - 68.4 - Qwen3-8B 76.0 67.3 - 62.0 - Phi-4-Reasoning-Plus-14B 81.3 78.0 53.6 69.3 - Gemini-2.5-Flash-Thinking-0520 82.3 72.0 64.2 82.8 62.3 o3-mini (medium) 79.6 76.7 53.3 76.8 65.9 DeepSeek-R1-0528-Qwen3-8B 86.0 76.3 61.5 61.1 60.5 3. Chat Website & API Platform You can chat with DeepSeek-R1 on DeepSeek's official website: chat.d",
                        "images": [],
                        "rank": 5
                      }
                    ],
                    "annotations": null,
                    "_meta": null
                  }
                ]
              }
            },
            "raw": {
              "deepseek_llm_new_releases_T002": {
                "content": [
                  {
                    "type": "text",
                    "text": [
                      {
                        "url": "https://www.cnbc.com/2025/09/30/whats-new-in-deepseeks-latest-model-deepseek-v3point2-exp.html",
                        "content": "What's new in DeepSeek's latest model: DeepSeek-V3.2-Exp Skip Navigation Markets Pre-Markets U.S. Markets Europe Markets China Markets Asia Markets World Markets Currencies Cryptocurrency Futures & Commodities Bonds Funds & ETFs Business Economy Finance Health & Science Media Real Estate Energy Climate Transportation Investigations Industrials Retail Wealth Sports Life Small Business Investing Personal Finance Fintech Financial Advisors Options Action ETF Street Buffett Archive Earnings Trader Talk Tech Cybersecurity AI Enterprise Internet Media Mobile Social Media CNBC Disruptor 50 Tech Guide Politics White House Policy Defense Congress Expanding Opportunity Europe Politics China Politics Asia Politics World Politics Video Latest Video Full Episodes Livestream Top Video Live Audio Europe TV Asia TV CNBC Podcasts CEO Interviews Digital Originals Watchlist Investing Club Trust Portfolio Analysis Trade Alerts Meeting Videos Homestretch Jim's Columns Education Subscribe PRO Pro News Josh Brown Mike Santoli Calls of the Day My Portfolio Livestream Full Episodes Stock Screener Market Forecast Options Investing Chart Investing Subscribe Livestream Menu Make It select USA INTL Livestream Search quotes, news & videos Livestream Watchlist SIGN IN Create free account Markets Business Investing Tech Politics Video Watchlist Investing Club PRO Livestream Menu Tech China's DeepSeek launches next-gen AI model. Here's what makes it different Published Tue, Sep 30 2025 4:43 AM EDT Updated Tue, Sep 30 2025 5:16 AM EDT Tasmin Lockwood WATCH LIVE Key Points DeepSeek announced on Monday the release of an experimental version of its current model DeepSeek-V3.1-Terminus. Despite speculation of a bubble forming, AI remains at the centre of geopolitical competition with the U.S. and China vying for the winning spot. \"This is DeepSeek's value prop all over: efficiency is becoming as important as raw power,\" according to Nick Patience, VP and Practice Lead for AI at The Futurum Group. Anna Barclay | Getty Images News | Getty Images Chinese startup DeepSeek's latest experimental model promises to increase efficiency and improve AI's ability to handle a lot of information at a fraction of the cost, but questions remain over how effective and safe the architecture is. DeepSeek sent Silicon Valley into a frenzy when it launched its first model R1 out of nowhere last year, showing that it's possible to train large language models (LLMs) quickly, on less powerful chips, using fewer resources. The company released DeepSeek-V3.2-Exp on Monday, an experimental version of its current model DeepSeek-V3.1-Terminus, which builds further on its mission to increase efficiency in AI systems, according to a post on the AI forum Hugging Face . \"DeepSeek V3.2 continues the focus on efficiency, cost reduction, and open-source sharing,\" Adina Yakefu, Chinese community lead at Hugging Face, told CNBC. \"The big improvement is a new feature called DSA (DeepSeek Sparse Attention), which makes the AI better at handling long documents and conversations. It also cuts the cost of running the AI in half compared to the previous version.\" \"It's significant because it should make the model faster and more cost-effective to use without a noticeable drop in performance,\" said Nick Patience, vice president and practice lead for AI at The Futurum Group. \"This makes powerful AI more accessible to developers, researchers, and smaller companies, potentially leading to a wave of new and innovative applications.\" The pros and cons of sparse attention An AI model makes decisions based on its training data and new information, such as a prompt. Say an airline wants to find the best route from A to B, while there are many options, not all are feasible. By filtering out the less viable routes, you dramatically reduce the amount of time, fuel and, ultimately, money, needed to make the journey. That is exactly sparse attention does, it only factors in data that it thinks is important given the task",
                        "images": [],
                        "rank": 1
                      },
                      {
                        "url": "https://llm-stats.com/ai-news",
                        "content": "LLM News Today (January 2026) Open Source LLM Updates & AI Model Releases llm-stats .com Leaderboards Arenas Benchmarks Compare News Advertise Search K Toggle theme Join our Discord Discuss models, benchmarks & more Join the community Join Follow us on X Latest AI news and updates Follow for updates Follow Subscribe to our Newsletter Weekly AI updates delivered to your inbox Stay updated Subscribe Advertise with us Reach 200k+ engineers, researchers and founders 200k+ monthly users Learn more Kimi K2 Base GPQA 0.5 Llama 3.1 Nemotron Nano 8B V1 GPQA 0.5 Claude 3 Haiku GPQA 0.3 GPT-4o GPQA 0.5 DeepSeek-V3.1 GPQA 0.7 DeepSeek R1 Zero GPQA 0.7 Qwen3 VL 8B Thinking GPQA 0.7 Kimi K2-Thinking-0905 GPQA 0.8 Gemini 3 Flash GPQA 0.9 GLM-4.5 GPQA 0.8 Qwen2 7B Instruct GPQA 0.3 MiMo-V2-Flash GPQA 0.8 Llama 3.1 405B Instruct GPQA 0.5 DeepSeek R1 Distill Llama 8B GPQA 0.5 Qwen2.5 7B Instruct GPQA 0.4 Phi-3.5-mini-instruct GPQA 0.3 Qwen3 VL 4B Thinking GPQA 0.6 GPT-5 High GPQA 0.9 Grok-2 mini GPQA 0.5 o1 GPQA 0.8 Kimi K2 Base GPQA 0.5 Llama 3.1 Nemotron Nano 8B V1 GPQA 0.5 Claude 3 Haiku GPQA 0.3 GPT-4o GPQA 0.5 DeepSeek-V3.1 GPQA 0.7 DeepSeek R1 Zero GPQA 0.7 Qwen3 VL 8B Thinking GPQA 0.7 Kimi K2-Thinking-0905 GPQA 0.8 Gemini 3 Flash GPQA 0.9 GLM-4.5 GPQA 0.8 Qwen2 7B Instruct GPQA 0.3 MiMo-V2-Flash GPQA 0.8 Llama 3.1 405B Instruct GPQA 0.5 DeepSeek R1 Distill Llama 8B GPQA 0.5 Qwen2.5 7B Instruct GPQA 0.4 Phi-3.5-mini-instruct GPQA 0.3 Qwen3 VL 4B Thinking GPQA 0.6 GPT-5 High GPQA 0.9 Grok-2 mini GPQA 0.5 o1 GPQA 0.8 AI News AI Trends LLM Updates Saturday, January 24 LLM News Today Your daily source for LLM news, open source LLM updates, and large language model news. Breaking announcements, new AI model releases, LLM benchmark news, and the latest updates from the AI industry. LLM News New Releases Open Source Benchmarks Resources FAQ Today Reddit 3m ago One-Minute Daily AI News 1/23/2026 Meta is stopping teens from chatting with its AI characters.[1] GitHub Releases Copilot-SDK to Embed Its Agentic Runtime in Any App.[2] Intel struggles to meet AI data center demand, shares drop 13%.[ Read article TechCrunch 4m ago Legal AI giant Harvey acquires Hexus as competition heats up in legal tech Hexus founder and CEO Sakshi Pratap, who previously held engineering roles at Walmart, Oracle, and Google, tells TechCrunch that her San Francisco-based team has already joined Harvey, while the start Read article Reddit 37m ago AI makes humans useless People who say AI makes humans useless are missing the point. Real data from Anthropics AI usage research shows the opposite, that users with higher skills and more education tend to get way better r Read article Yesterday TechXplore 10h ago The next generation of disinformation: AI swarms can threaten democracy by manufacturing fake public consensus An international research team involving Konstanz scientist David Garcia warns that the next generation of influence operations may not look like obvious \"copy-paste bots,\" but like coordinated commun Read article Reddit 11h ago AMD Ryzen AI Software 1.7 released for improved performance on NPUs, new model support submitted by /u/Fcking_Chuck [link] [comments] Read article TechXplore 12h ago Stress-testing AI vision systems: Rethinking how adversarial images are generated Deep neural networks (DNNs) have become a cornerstone of modern AI technology, driving a thriving field of research in image-related tasks. These systems have found applications in medical diagnosis, Read article TechCrunch 12h ago Meta pauses teen access to AI characters ahead of new version Meta today said that it is pausing teens' access to its AI characters globally across all its apps. The company mentioned that it is not abandoning its efforts but wants to develop an updated version Read article TechCrunch 13h ago AI CEOs transformed Davos into a tech conference The World Economic Forums annual meeting in Davos felt different this",
                        "images": [],
                        "rank": 2
                      },
                      {
                        "url": "https://api-docs.deepseek.com/news/news251201",
                        "content": "DeepSeek-V3.2 Release | DeepSeek API Docs Skip to main content DeepSeek API Docs English English DeepSeek Platform Quick Start Your First API Call Models & Pricing The Temperature Parameter Token & Token Usage Rate Limit Error Codes News DeepSeek-V3.2 Release 2025/12/01 DeepSeek-V3.2-Exp Release 2025/09/29 DeepSeek V3.1 Update 2025/09/22 DeepSeek V3.1 Release 2025/08/21 DeepSeek-R1-0528 Release 2025/05/28 DeepSeek-V3-0324 Release 2025/03/25 DeepSeek-R1 Release 2025/01/20 DeepSeek APP 2025/01/15 Introducing DeepSeek-V3 2024/12/26 DeepSeek-V2.5-1210 Release 2024/12/10 DeepSeek-R1-Lite Release 2024/11/20 DeepSeek-V2.5 Release 2024/09/05 Context Caching is Available 2024/08/02 New API Features 2024/07/25 API Reference API Guides Thinking Mode Multi-round Conversation Chat Prefix Completion (Beta) FIM Completion (Beta) JSON Output Tool Calls Context Caching Anthropic API Other Resources Integrations API Status Page FAQ Change Log News DeepSeek-V3.2 Release 2025/12/01 DeepSeek-V3.2 Release Launching DeepSeek-V3.2 & DeepSeek-V3.2-Speciale Reasoning-first models built for agents! DeepSeek-V3.2: Official successor to V3.2-Exp. Now live on App, Web & API. DeepSeek-V3.2-Speciale: Pushing the boundaries of reasoning capabilities. API-only for now. Tech report: https://huggingface.co/deepseek-ai/DeepSeek-V3.2/resolve/main/assets/paper.pdf World-Leading Reasoning V3.2: Balanced inference vs. length. Your daily driver at GPT-5 level performance. V3.2-Speciale: Maxed-out reasoning capabilities. Rivals Gemini-3.0-Pro. Gold-Medal Performance: V3.2-Speciale attains gold-level results in IMO, CMO, ICPC World Finals & IOI 2025. Note: V3.2-Speciale dominates complex tasks but requires higher token usage. Currently API-only (no tool-use) to support community evaluation & research. Thinking in Tool-Use Introduces a new massive agent training data synthesis method covering 1,800+ environments & 85k+ complex instructions. DeepSeek-V3.2 is our first model to integrate thinking directly into tool-use, and also supports tool-use in both thinking and non-thinking modes. API Update V3.2: Same usage pattern as V3.2-Exp. V3.2-Speciale: Served via a temporary endpoint: base_url=\" https://api.deepseek.com/v3.2_speciale_expires_on_20251215 \". Same pricing as V3.2, no tool calls, available until Dec 15th, 2025, 15:59 (UTC Time). V3.2 now supports Thinking in Tool-Use details: https://api-docs.deepseek.com/guides/thinking_mode Open Source Release DeepSeek-V3.2 Model: https://huggingface.co/deepseek-ai/DeepSeek-V3.2 DeepSeek-V3.2-Speciale Model: https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale Tech report: https://huggingface.co/deepseek-ai/DeepSeek-V3.2/resolve/main/assets/paper.pdf Previous Error Codes Next Introducing DeepSeek-V3.2-Exp WeChat Official Account Community Email Discord Twitter More GitHub Copyright 2025 DeepSeek, Inc.",
                        "images": [],
                        "rank": 3
                      },
                      {
                        "url": "https://www.producthunt.com/products/deepseek/launches",
                        "content": "DeepSeek Latest Launches (2026) | Product Hunt Best Products Launches Launch archive Most-loved launches by the community Launch Guide Checklists and pro tips for launching News Newsletter The best of Product Hunt, every day Stories Tech news, interviews, and tips from makers Changelog New Product Hunt features and releases Forums Forums Ask questions, find support, and connect Streaks The most active community members Events Meet others online and in-person Advertise Subscribe Sign in DeepSeek Open-source LLM optimized for advanced reasoning and code 4.8 40 reviews 2.1K followers Open-source LLM optimized for advanced reasoning and code 4.8 40 reviews 2.1K followers Visit website LLMs AI Chatbots Foundation Models Intelligent assistant for coding, content creation, file reading, and more. Upload documents, engage in extended conversations, and receive expert assistance in AI, natural language processing, and beyond. Overview Launches 12 Reviews 40 Alternatives Customers Built with Forum More DeepSeek launches Launch date DeepSeek V3.2 Open-Source LLM matching GPT-5 Launched on December 2nd, 2025 6 55 DeepSeek-V3.2 Reasoning-first models built for agents Launched on December 2nd, 2025 2 205 DeepSeekMath-V2 IMO Gold level reasoning, fully open. Launched on November 28th, 2025 1 16 DeepSeek-OCR Read documents like an image Launched on October 21st, 2025 11 363 DeepSeek-V3.2-Exp Long-context efficiency with DeepSeek Sparse Attention Launched on September 30th, 2025 2 171 DeepSeek-V3.1-Terminus A refined agentic model for developers Launched on September 23rd, 2025 6 156 DeepSeek-V3.1 Our first step toward the agent era Launched on August 22nd, 2025 9 438 DeepSeek-R1-0528 New open-source LLM that rivals o3 in coding & reasoning Launched on May 29th, 2025 6 230 DeepSeek For Android DeepSeek's official AI assistant for free Launched on January 17th, 2025 1 36 DeepSeek for iOS Your Al assistant powered by DeepSeek-V3 Launched on January 15th, 2025 9 373 Follow DeepSeek Add to collection Share Company Info chat.deepseek.com/coder App Store DeepSeek Info Launched in 2024 View 12 launches Forum p/deepseek Awards View all Similar Products Claude by Anthropic A family of foundational AI models 5.0 ( 594 reviews ) LLMs AI Chatbots OpenAI APIs and tools for building AI products 5.0 ( 672 reviews ) LLMs AI Chatbots ChatGPT by OpenAI Get answers. Find inspiration. Be more productive. 4.8 ( 638 reviews ) LLMs AI Chatbots Gemini Google's answer to GPT-4 4.8 ( 120 reviews ) LLMs AI Chatbots Mistral AI Open and portable generative AI for devs and businesses 5.0 ( 32 reviews ) LLMs AI Chatbots View more Subscribe Sign in Social X Top Product Categories Engineering & Development LLMs Productivity Marketing & Sales Design & Creative Social & Community Finance AI Agents Engineering & Development Vibe Coding Tools AI Coding Agents AI Code Editors LLMs AI Chatbots AI Infrastructure Tools Prompt Engineering Tools Productivity AI notetakers Note and writing apps Team collaboration software Search Marketing & Sales Lead generation software Marketing automation platforms Design & Creative Video editing Design resources Graphic design tools AI Generative Media Social & Community Social Networking Professional networking platforms Community management Finance Accounting software Fundraising resources Investing AI Agents AI Voice Agents AI Agent Automation See All Categories >> Trending categories Top reviewed Trending products Top forum threads Trending categories Vibe Coding Tools AI Dictation Apps AI notetakers Code Review Tools No-code Platforms Figma Plugins Static site generators Top reviewed Lovable n8n Attio PostHog Vapi Granola Raycast Supabase Trending products Lovable Screen Studio bolt.new Wispr Flow Framer Replit Vapi Granola Top forum threads Cursor or Claude Code? POLL: Domain or product first? YC deadline in <2 weeks; Who's applying? We Got into YC, Got Kicked Out, and Fought Our Way Back How Wispr Flow found PMF through pivot Best Vibe",
                        "images": [],
                        "rank": 4
                      },
                      {
                        "url": "https://huggingface.co/deepseek-ai/DeepSeek-R1-0528",
                        "content": "deepseek-ai/DeepSeek-R1-0528 Hugging Face Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up deepseek-ai / DeepSeek-R1-0528 like 2.39k Follow DeepSeek 115k Text Generation Transformers Safetensors deepseek_v3 conversational custom_code text-generation-inference fp8 arxiv: 2501.12948 License: mit Model card Files Files and versions xet Community 113 Deploy Use this model DeepSeek-R1-0528 1. Introduction 2. Evaluation Results DeepSeek-R1-0528 DeepSeek-R1-0528-Qwen3-8B 3. Chat Website & API Platform 4. How to Run Locally System Prompt Temperature Prompts for File Uploading and Web Search 5. License 6. Citation 7. Contact DeepSeek-R1-0528 Paper Link 1. Introduction The DeepSeek R1 model has undergone a minor version upgrade, with the current version being DeepSeek-R1-0528. In the latest update, DeepSeek R1 has significantly improved its depth of reasoning and inference capabilities by leveraging increased computational resources and introducing algorithmic optimization mechanisms during post-training. The model has demonstrated outstanding performance across various benchmark evaluations, including mathematics, programming, and general logic. Its overall performance is now approaching that of leading models, such as O3 and Gemini 2.5 Pro. Compared to the previous version, the upgraded model shows significant improvements in handling complex reasoning tasks. For instance, in the AIME 2025 test, the models accuracy has increased from 70% in the previous version to 87.5% in the current version. This advancement stems from enhanced thinking depth during the reasoning process: in the AIME test set, the previous model used an average of 12K tokens per question, whereas the new version averages 23K tokens per question. Beyond its improved reasoning capabilities, this version also offers a reduced hallucination rate, enhanced support for function calling, and better experience for vibe coding. 2. Evaluation Results DeepSeek-R1-0528 For all our models, the maximum generation length is set to 64K tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 16 responses per query to estimate pass@1. Category Benchmark (Metric) DeepSeek R1 DeepSeek R1 0528 General MMLU-Redux (EM) 92.9 93.4 MMLU-Pro (EM) 84.0 85.0 GPQA-Diamond (Pass@1) 71.5 81.0 SimpleQA (Correct) 30.1 27.8 FRAMES (Acc.) 82.5 83.0 Humanity's Last Exam (Pass@1) 8.5 17.7 Code LiveCodeBench (2408-2505) (Pass@1) 63.5 73.3 Codeforces-Div1 (Rating) 1530 1930 SWE Verified (Resolved) 49.2 57.6 Aider-Polyglot (Acc.) 53.3 71.6 Math AIME 2024 (Pass@1) 79.8 91.4 AIME 2025 (Pass@1) 70.0 87.5 HMMT 2025 (Pass@1) 41.7 79.4 CNMO 2024 (Pass@1) 78.8 86.9 Tools BFCL_v3_MultiTurn (Acc) - 37.0 Tau-Bench  (Pass@1) - 53.5(Airline)/63.9(Retail) Note: We use Agentless framework to evaluate model performance on SWE-Verified. We only evaluate text-only prompts in HLE testsets. GPT-4.1 is employed to act user role in Tau-bench evaluation. DeepSeek-R1-0528-Qwen3-8B Meanwhile, we distilled the chain-of-thought from DeepSeek-R1-0528 to post-train Qwen3 8B Base, obtaining DeepSeek-R1-0528-Qwen3-8B. This model achieves state-of-the-art (SOTA) performance among open-source models on the AIME 2024, surpassing Qwen3 8B by +10.0% and matching the performance of Qwen3-235B-thinking. We believe that the chain-of-thought from DeepSeek-R1-0528 will hold significant importance for both academic research on reasoning models and industrial development focused on small-scale models. AIME 24 AIME 25 HMMT Feb 25 GPQA Diamond LiveCodeBench (2408-2505) Qwen3-235B-A22B 85.7 81.5 62.5 71.1 66.5 Qwen3-32B 81.4 72.9 - 68.4 - Qwen3-8B 76.0 67.3 - 62.0 - Phi-4-Reasoning-Plus-14B 81.3 78.0 53.6 69.3 - Gemini-2.5-Flash-Thinking-0520 82.3 72.0 64.2 82.8 62.3 o3-mini (medium) 79.6 76.7 53.3 76.8 65.9 DeepSeek-R1-0528-Qwen3-8B 86.0 76.3 61.5 61.1 60.5 3. Chat Website & API Platform You can chat with DeepSeek-R1 on DeepSeek's official website: chat.d",
                        "images": [],
                        "rank": 5
                      }
                    ],
                    "annotations": null,
                    "_meta": null
                  }
                ]
              }
            },
            "logs": "\n\u001b[1m  RetrieverAgent finished\u001b[0m\n{'output_keys': ['kashmir_weather_data_T001', 'call_self', 'code_variants']}\n\n\u001b[1m \u001b[0m\u001b[1m[\u001b[0m\u001b[1mCODE:\u001b[0m\u001b[1m]\u001b[0m\u001b[1m: kashmir_weather_data_T001 = \u001b[0m\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n\u001b[1mtotal_tokens = \u001b[0m\u001b[1;36m185\u001b[0m\n\u001b[1mexecuted_model = \u001b[0m\u001b[1;32m'gemini:gemini-2.5-flash-lite'\u001b[0m\n\u001b[1mreads_data = \u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n\u001b[1mweather_data = \u001b[0m\u001b[1;35mjson.loads\u001b[0m\u001b[1m(\u001b[0m\u001b[1;35msearch_web_with_text_content\u001b[0m\u001b[1m(\u001b[0m\u001b[1;32m'Kashmir current weather \u001b[0m\n\u001b[1;32mtemperature conditions precipitation wind speed'\u001b[0m\u001b[1m, \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n\u001b[1mreturn \u001b[0m\u001b[1m{\u001b[0m\u001b[1;32m'kashmir_weather_data_T001'\u001b[0m\u001b[1m: weather_data\u001b[0m\u001b[1m}\u001b[0m\n",
            "execution_time": "2026-01-24 11:30:14",
            "total_time": "9.355",
            "executed_variant": "CODE_1A"
          }
        }
      ],
      "input_tokens": 198,
      "output_tokens": 45,
      "total_tokens": 243
    },
    {
      "id": "T003",
      "description": "Synthesize and combine all gathered information about DeepSeek's LLM activities.",
      "agent": "ThinkerAgent",
      "agent_prompt": "Combine the information from 'deepseek_llm_innovations_research_T001' and 'deepseek_llm_new_releases_T002'. Identify overlapping themes, consolidate similar findings, and structure the data logically for a comprehensive update. Ensure all unique and relevant pieces of information are retained.",
      "reads": [
        "deepseek_llm_innovations_research_T001",
        "deepseek_llm_new_releases_T002"
      ],
      "writes": [
        "combined_deepseek_llm_data_T003"
      ],
      "status": "completed",
      "output": {
        "combined_deepseek_llm_data_T003": {
          "innovations": [
            {
              "name": "Engram Module",
              "description": "A new conditional memory architecture designed to address Transformer's fundamental memory limitations. It provides O(1) knowledge lookup for static patterns by decoupling knowledge storage and retrieval from intensive computation. This frees up LLM layers for deeper reasoning.",
              "key_features": [
                "O(1) knowledge lookup",
                "Conditional memory",
                "New axis of sparsity",
                "Complements Mixture of Experts (MoE)",
                "Tokenizer Compression (23% vocabulary reduction)",
                "Multi-Head Hashing",
                "Context-Aware Gating"
              ],
              "benefits": [
                "Fixes Transformer's memory flaws",
                "Reduces compute waste for static knowledge retrieval",
                "Boosts reasoning capabilities",
                "Enables scaling of LLMs",
                "Reduces GPU memory bottleneck",
                "Potential for improved long-context accuracy (97% demonstrated)",
                "Offloads 100B parameters to CPU/SSD with <3% overhead"
              ],
              "associated_paper": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
              "release_date_mentioned": "January 12, 2026",
              "affiliation": "DeepSeek AI (collaboration with Peking University)"
            }
          ],
          "models_and_research": [
            {
              "name": "DeepSeek V3",
              "type": "LLM",
              "key_aspects": "Powered by Mixture of Experts (MoE) architecture."
            },
            {
              "name": "DeepSeek Engram V4 Architecture",
              "type": "Potential Architecture",
              "key_aspects": "Hinted to be underpinned by the Engram module."
            },
            {
              "name": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
              "type": "Research Paper",
              "publication_date": "January 5, 2024"
            },
            {
              "name": "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models",
              "type": "Research Paper",
              "publication_date": "January 11, 2024"
            },
            {
              "name": "DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence",
              "type": "LLM / Research Paper",
              "publication_date": "January 25, 2024"
            },
            {
              "name": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
              "type": "LLM / Research Paper",
              "publication_date": "February 5, 2024"
            },
            {
              "name": "DeepSeek-VL: Towards Real-World Vision-Language Understanding",
              "type": "LLM / Research Paper",
              "publication_date": "March 8, 2024"
            },
            {
              "name": "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model",
              "type": "LLM / Research Paper",
              "publication_date": "May 7, 2024"
            },
            {
              "name": "DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data",
              "type": "LLM / Research Paper",
              "publication_date": "May 23, 2024"
            },
            {
              "name": "DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence",
              "type": "LLM / Research Paper",
              "publication_date": "June 17, 2024"
            },
            {
              "name": "Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models",
              "type": "Research Paper",
              "publication_date": "July 2, 2024"
            },
            {
              "name": "DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search",
              "type": "LLM / Research Paper",
              "publication_date": "August 15, 2024"
            },
            {
              "name": "Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep Learning",
              "type": "Research Paper",
              "publication_date": "August 26, 2024"
            },
            {
              "name": "Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts",
              "type": "Research Paper",
              "publication_date": "August 28, 2024"
            },
            {
              "name": "Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation",
              "type": "Research Paper",
              "publication_date": "October 17, 2024"
            },
            {
              "name": "JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation",
              "type": "Research Paper",
              "publication_date": "November 12, 2024"
            },
            {
              "name": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding",
              "type": "LLM / Research Paper",
              "publication_date": "December 13, 2024"
            },
            {
              "name": "DeepSeek-V3 Technical Report",
              "type": "Research Paper",
              "publication_date": "December 27, 2024"
            },
            {
              "name": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
              "type": "LLM / Research Paper",
              "publication_date": null
            }
          ],
          "key_themes": [
            {
              "theme": "Architectural Innovations for LLMs",
              "details": "DeepSeek is actively researching and releasing novel architectures to overcome fundamental limitations in LLMs. The Engram module represents a significant step in separating knowledge retrieval from computational reasoning, offering O(1) lookup and reducing reliance on dense computation for static information. This directly addresses Transformer's memory inefficiency and GPU memory bottlenecks."
            },
            {
              "theme": "Advancements in Mixture-of-Experts (MoE)",
              "details": "MoE remains a core focus, with research exploring expert specialization (DeepSeekMoE, Let the Expert Stick to His Last), and efficient load balancing (Auxiliary-Loss-Free Load Balancing Strategy). DeepSeek-V2 and DeepSeek-VL2 are examples of MoE models."
            },
            {
              "theme": "Specialized LLMs for Code and Mathematics",
              "details": "DeepSeek is developing and releasing models tailored for specific domains, such as DeepSeek-Coder (and its V2 iteration) for code intelligence and DeepSeekMath for mathematical reasoning. They also focus on theorem proving with DeepSeek-Prover."
            },
            {
              "theme": "Multimodal Understanding and Generation",
              "details": "Research extends to vision-language capabilities with DeepSeek-VL and DeepSeek-VL2, and unified multimodal processing through models like Janus and JanusFlow."
            },
            {
              "theme": "Performance and Efficiency",
              "details": "A recurring theme is improving model performance (benchmarks, accuracy) while maintaining efficiency (economical, cost-effective, reduced overhead). Engram's ability to offload to CPU/SSD and optimize memory allocation contributes to this."
            },
            {
              "theme": "Open-Source Contributions",
              "details": "DeepSeek emphasizes open-source contributions, with papers and code for various models and research areas being made available, fostering community engagement (e.g., GitHub stars for Engram)."
            }
          ],
          "gap_analysis": [
            {
              "area": "New Releases (T002 Input)",
              "observation": "The input for 'deepseek_llm_new_releases_T002' indicated an error and no search results were found.",
              "implication": "No specific new *product* releases or announcements beyond research papers could be identified from this source. The combination relies heavily on the 'innovations_research' input for understanding DeepSeek's forward-looking developments."
            },
            {
              "area": "Engram V4 Specifics",
              "observation": "While Engram is presented as a key architectural innovation potentially underpinning DeepSeek V4, concrete details or a formal release announcement for 'DeepSeek V4' itself are not explicitly detailed in the provided texts. The association is speculative ('Could This Be DeepSeek V4?').",
              "implication": "The exact launch or specifications of DeepSeek V4 remain to be confirmed, though Engram is positioned as a foundational element."
            }
          ],
          "comparison_notes": [
            "The 'deepseek_llm_innovations_research_T001' provides in-depth technical details on the Engram module, its architecture, benefits, and the underlying research paper, while also listing a comprehensive set of DeepSeek's past research publications and model releases.",
            "The 'deepseek_llm_new_releases_T002' was effectively empty, meaning no new product releases were identified from that source. Therefore, the primary source of information for combining and structuring remains 'deepseek_llm_innovations_research_T001'.",
            "Engram is presented as a significant architectural innovation that directly addresses limitations found in existing Transformer and MoE architectures, suggesting it's a key enabler for future DeepSeek models, potentially including 'DeepSeek V4'."
          ]
        },
        "cost": 0.00092,
        "input_tokens": 3748,
        "output_tokens": 1363,
        "total_tokens": 5111,
        "executed_model": "gemini:gemini-2.5-flash-lite"
      },
      "error": null,
      "cost": 0.00092,
      "start_time": "2026-01-24T06:01:37.366133",
      "end_time": "2026-01-24T06:01:45.250126",
      "execution_time": 7.883993,
      "iterations": [
        {
          "iteration": 1,
          "output": {
            "combined_deepseek_llm_data_T003": {
              "innovations": [
                {
                  "name": "Engram Module",
                  "description": "A new conditional memory architecture designed to address Transformer's fundamental memory limitations. It provides O(1) knowledge lookup for static patterns by decoupling knowledge storage and retrieval from intensive computation. This frees up LLM layers for deeper reasoning.",
                  "key_features": [
                    "O(1) knowledge lookup",
                    "Conditional memory",
                    "New axis of sparsity",
                    "Complements Mixture of Experts (MoE)",
                    "Tokenizer Compression (23% vocabulary reduction)",
                    "Multi-Head Hashing",
                    "Context-Aware Gating"
                  ],
                  "benefits": [
                    "Fixes Transformer's memory flaws",
                    "Reduces compute waste for static knowledge retrieval",
                    "Boosts reasoning capabilities",
                    "Enables scaling of LLMs",
                    "Reduces GPU memory bottleneck",
                    "Potential for improved long-context accuracy (97% demonstrated)",
                    "Offloads 100B parameters to CPU/SSD with <3% overhead"
                  ],
                  "associated_paper": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
                  "release_date_mentioned": "January 12, 2026",
                  "affiliation": "DeepSeek AI (collaboration with Peking University)"
                }
              ],
              "models_and_research": [
                {
                  "name": "DeepSeek V3",
                  "type": "LLM",
                  "key_aspects": "Powered by Mixture of Experts (MoE) architecture."
                },
                {
                  "name": "DeepSeek Engram V4 Architecture",
                  "type": "Potential Architecture",
                  "key_aspects": "Hinted to be underpinned by the Engram module."
                },
                {
                  "name": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
                  "type": "Research Paper",
                  "publication_date": "January 5, 2024"
                },
                {
                  "name": "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models",
                  "type": "Research Paper",
                  "publication_date": "January 11, 2024"
                },
                {
                  "name": "DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence",
                  "type": "LLM / Research Paper",
                  "publication_date": "January 25, 2024"
                },
                {
                  "name": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
                  "type": "LLM / Research Paper",
                  "publication_date": "February 5, 2024"
                },
                {
                  "name": "DeepSeek-VL: Towards Real-World Vision-Language Understanding",
                  "type": "LLM / Research Paper",
                  "publication_date": "March 8, 2024"
                },
                {
                  "name": "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model",
                  "type": "LLM / Research Paper",
                  "publication_date": "May 7, 2024"
                },
                {
                  "name": "DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data",
                  "type": "LLM / Research Paper",
                  "publication_date": "May 23, 2024"
                },
                {
                  "name": "DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence",
                  "type": "LLM / Research Paper",
                  "publication_date": "June 17, 2024"
                },
                {
                  "name": "Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models",
                  "type": "Research Paper",
                  "publication_date": "July 2, 2024"
                },
                {
                  "name": "DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search",
                  "type": "LLM / Research Paper",
                  "publication_date": "August 15, 2024"
                },
                {
                  "name": "Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep Learning",
                  "type": "Research Paper",
                  "publication_date": "August 26, 2024"
                },
                {
                  "name": "Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts",
                  "type": "Research Paper",
                  "publication_date": "August 28, 2024"
                },
                {
                  "name": "Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation",
                  "type": "Research Paper",
                  "publication_date": "October 17, 2024"
                },
                {
                  "name": "JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation",
                  "type": "Research Paper",
                  "publication_date": "November 12, 2024"
                },
                {
                  "name": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding",
                  "type": "LLM / Research Paper",
                  "publication_date": "December 13, 2024"
                },
                {
                  "name": "DeepSeek-V3 Technical Report",
                  "type": "Research Paper",
                  "publication_date": "December 27, 2024"
                },
                {
                  "name": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
                  "type": "LLM / Research Paper",
                  "publication_date": null
                }
              ],
              "key_themes": [
                {
                  "theme": "Architectural Innovations for LLMs",
                  "details": "DeepSeek is actively researching and releasing novel architectures to overcome fundamental limitations in LLMs. The Engram module represents a significant step in separating knowledge retrieval from computational reasoning, offering O(1) lookup and reducing reliance on dense computation for static information. This directly addresses Transformer's memory inefficiency and GPU memory bottlenecks."
                },
                {
                  "theme": "Advancements in Mixture-of-Experts (MoE)",
                  "details": "MoE remains a core focus, with research exploring expert specialization (DeepSeekMoE, Let the Expert Stick to His Last), and efficient load balancing (Auxiliary-Loss-Free Load Balancing Strategy). DeepSeek-V2 and DeepSeek-VL2 are examples of MoE models."
                },
                {
                  "theme": "Specialized LLMs for Code and Mathematics",
                  "details": "DeepSeek is developing and releasing models tailored for specific domains, such as DeepSeek-Coder (and its V2 iteration) for code intelligence and DeepSeekMath for mathematical reasoning. They also focus on theorem proving with DeepSeek-Prover."
                },
                {
                  "theme": "Multimodal Understanding and Generation",
                  "details": "Research extends to vision-language capabilities with DeepSeek-VL and DeepSeek-VL2, and unified multimodal processing through models like Janus and JanusFlow."
                },
                {
                  "theme": "Performance and Efficiency",
                  "details": "A recurring theme is improving model performance (benchmarks, accuracy) while maintaining efficiency (economical, cost-effective, reduced overhead). Engram's ability to offload to CPU/SSD and optimize memory allocation contributes to this."
                },
                {
                  "theme": "Open-Source Contributions",
                  "details": "DeepSeek emphasizes open-source contributions, with papers and code for various models and research areas being made available, fostering community engagement (e.g., GitHub stars for Engram)."
                }
              ],
              "gap_analysis": [
                {
                  "area": "New Releases (T002 Input)",
                  "observation": "The input for 'deepseek_llm_new_releases_T002' indicated an error and no search results were found.",
                  "implication": "No specific new *product* releases or announcements beyond research papers could be identified from this source. The combination relies heavily on the 'innovations_research' input for understanding DeepSeek's forward-looking developments."
                },
                {
                  "area": "Engram V4 Specifics",
                  "observation": "While Engram is presented as a key architectural innovation potentially underpinning DeepSeek V4, concrete details or a formal release announcement for 'DeepSeek V4' itself are not explicitly detailed in the provided texts. The association is speculative ('Could This Be DeepSeek V4?').",
                  "implication": "The exact launch or specifications of DeepSeek V4 remain to be confirmed, though Engram is positioned as a foundational element."
                }
              ],
              "comparison_notes": [
                "The 'deepseek_llm_innovations_research_T001' provides in-depth technical details on the Engram module, its architecture, benefits, and the underlying research paper, while also listing a comprehensive set of DeepSeek's past research publications and model releases.",
                "The 'deepseek_llm_new_releases_T002' was effectively empty, meaning no new product releases were identified from that source. Therefore, the primary source of information for combining and structuring remains 'deepseek_llm_innovations_research_T001'.",
                "Engram is presented as a significant architectural innovation that directly addresses limitations found in existing Transformer and MoE architectures, suggesting it's a key enabler for future DeepSeek models, potentially including 'DeepSeek V4'."
              ]
            },
            "cost": 0.00092,
            "input_tokens": 3748,
            "output_tokens": 1363,
            "total_tokens": 5111,
            "executed_model": "gemini:gemini-2.5-flash-lite"
          }
        }
      ],
      "input_tokens": 3748,
      "output_tokens": 1363,
      "total_tokens": 5111
    },
    {
      "id": "T004",
      "description": "Format the synthesized DeepSeek LLM update into a Markdown file.",
      "agent": "FormatterAgent",
      "agent_prompt": "Take the consolidated LLM information from 'combined_deepseek_llm_data_T003' and format it into a detailed, comprehensive Markdown report. Organize the content clearly with headings for innovations, research, and new releases. Ensure the output is suitable for saving as 'DeepSeek_Update.md'. Aim for a detailed and well-structured report, not a concise summary.",
      "reads": [
        "combined_deepseek_llm_data_T003"
      ],
      "writes": [
        "formatted_deepseek_update_T004"
      ],
      "status": "completed",
      "output": {
        "final_format": "markdown",
        "markdown_report": "# DeepSeek LLM Innovations and Research Update (January 2026)\n\nThis report details the latest advancements and research from DeepSeek in the field of Large Language Models (LLMs), focusing on architectural innovations, key research themes, and recent developments as of January 2026.\n\n## 1. Executive Summary\n\nDeepSeek AI, in collaboration with Peking University, has unveiled a significant architectural innovation with the **Engram module**, designed to address fundamental memory limitations inherent in Transformer-based LLMs. This novel conditional memory architecture promises O(1) knowledge lookup, decoupling static knowledge retrieval from intensive computation and freeing up LLM layers for more profound reasoning. This advancement is poised to be a cornerstone for future DeepSeek models, potentially powering the anticipated DeepSeek V4. Beyond Engram, DeepSeek continues its strong trajectory in developing specialized LLMs for code and mathematics, advancing multimodal capabilities, and refining Mixture-of-Experts (MoE) architectures, all while maintaining a commitment to open-source contributions.\n\n## 2. Core Innovation: The Engram Module\n\nThe Engram module represents a paradigm shift in LLM architecture, directly tackling the inefficiencies of static knowledge retrieval within the Transformer framework. Unlike traditional LLMs that simulate knowledge lookup through computationally expensive multi-layer processing, Engram provides a deterministic, constant-time (O(1)) mechanism for accessing factual information. This is achieved by decoupling knowledge storage and retrieval from the core computational layers, thereby reducing compute waste and alleviating GPU memory bottlenecks.\n\n### 2.1. Problem Addressed: Transformer's Memory Limitation\n\nModern LLMs, including DeepSeek V3, predominantly utilize the Transformer architecture, which, despite its power, lacks a native mechanism for efficient knowledge lookup. Tasks that ideally require a simple dictionary-like retrieval, such as identifying a known entity or a common phrase, are instead processed through extensive computations across multiple layers. This is fundamentally wasteful, as it consumes valuable computational resources that could otherwise be dedicated to complex reasoning and problem-solving.\n\n### 2.2. Engram's Architecture and Functionality\n\nThe Engram module introduces a new axis of sparsity, augmenting existing LLM architectures. It acts as a complementary component, leveraging lessons from classical N-gram embeddings. The core of Engram's functionality lies in its ability to store static patternslike named entities, fixed phrases, and formulasin a dedicated embedding matrix. This matrix is learned during the training process. During inference, these patterns are retrieved using computationally inexpensive lookup operations.\n\nKey features of the Engram module include:\n\n*   **O(1) Knowledge Lookup**: Enables constant-time retrieval of static knowledge.\n*   **Conditional Memory**: Memory access is conditional, guided by the model's current state.\n*   **New Axis of Sparsity**: Introduces a distinct dimension of sparsity beyond computational sparsity.\n*   **Complementary to MoE**: Works in conjunction with Mixture-of-Experts (MoE) architectures.\n*   **Tokenizer Compression**: Reduces vocabulary size by 23% by mapping equivalent tokens to canonical forms.\n*   **Multi-Head Hashing**: Employs multiple hash heads to mitigate collisions when storing N-grams.\n*   **Context-Aware Gating**: A gating mechanism uses the model's hidden state to query retrieved memory, ensuring contextual relevance.\n\n### 2.3. Benefits and Impact\n\nThe implementation of the Engram module offers a multitude of benefits:\n\n*   **Fixes Transformer's Memory Flaws**: Directly addresses the inefficiency of simulating knowledge retrieval through computation.\n*   **Reduces Compute Waste**: Minimizes unnecessary computational cycles spent on recalling static information.\n*   **Boosts Reasoning Capabilities**: Frees up LLM layers to focus on deeper, more complex reasoning tasks.\n*   **Enables LLM Scaling**: Helps overcome memory bottlenecks that limit the scalability of LLMs.\n*   **Reduces GPU Memory Bottleneck**: Optimizes memory usage, particularly crucial given the scarcity and cost of GPU High-Bandwidth Memory (HBM).\n*   **Improved Long-Context Accuracy**: Demonstrated potential for improved accuracy in handling long contexts, achieving 97% in specific tests.\n*   **Efficient Offloading**: Capable of offloading a significant number of parameters (e.g., 100 billion) to CPU/SSD with minimal (<3%) overhead.\n\nThe research paper associated with this innovation is titled \"Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models,\" and it was released on January 12, 2026. This development is a significant collaboration between DeepSeek AI and Peking University.\n\n## 3. DeepSeek's Research and Model Portfolio\n\nDeepSeek has a robust and diverse research agenda, reflected in its extensive portfolio of published papers and developed models. The following outlines key areas and notable contributions:\n\n### 3.1. Architectural Innovations & MoE Advancements\n\n*   **DeepSeek V3**: A prominent LLM powered by the Mixture of Experts (MoE) architecture.\n*   **DeepSeek Engram V4 Architecture**: The Engram module is strongly hinted to be the foundational architecture for the forthcoming DeepSeek V4, representing a significant evolution.\n*   **DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models** (Published: January 11, 2024): Explores enhancing MoE models through specialized experts.\n*   **Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models** (Published: July 2, 2024): Further research into expert specialization for sparse LLM architectures.\n*   **Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts** (Published: August 28, 2024): Focuses on improving the efficiency of MoE models through advanced load balancing.\n\n### 3.2. Specialized LLMs for Domain Expertise\n\n*   **DeepSeek-Coder**: Developed for code intelligence, with its second iteration, **DeepSeek-Coder-V2** (Published: June 17, 2024), aiming to compete with closed-source models.\n*   **DeepSeekMath**: Pushing the boundaries of mathematical reasoning in open language models (Published: February 5, 2024).\n*   **DeepSeek-Prover**: Advancing theorem proving capabilities in LLMs through large-scale synthetic data (Published: May 23, 2024), with an updated version **DeepSeek-Prover-V1.5** (Published: August 15, 2024) utilizing proof assistant feedback.\n\n### 3.3. Multimodal Understanding and Generation\n\n*   **DeepSeek-VL**: Focuses on real-world vision-language understanding (Published: March 8, 2024).\n*   **Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation** (Published: October 17, 2024) and **JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation** (Published: November 12, 2024): These research efforts aim for unified multimodal processing.\n*   **DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding** (Published: December 13, 2024): Integrates MoE into vision-language models for enhanced capabilities.\n\n### 3.4. Performance, Efficiency, and Broader Research\n\n*   **DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model** (Published: May 7, 2024): Highlights efficiency alongside performance in MoE models.\n*   **Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep Learning** (Published: August 26, 2024): Explores cost-effective solutions for AI hardware.\n*   **DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning**: A research direction focused on enhancing reasoning through RL.\n*   **DeepSeek-V3 Technical Report** (Published: December 27, 2024): Provides comprehensive details on the DeepSeek V3 model.\n\n### 3.5. Foundational Research\n\n*   **DeepSeek LLM: Scaling Open-Source Language Models with Longtermism** (Published: January 5, 2024): A foundational paper outlining strategies for scaling open-source LLMs.\n\n## 4. Key Themes in DeepSeek's Development\n\nSeveral overarching themes define DeepSeek's strategic direction in LLM research and development:\n\n*   **Architectural Innovation for LLMs**: A consistent drive to overcome fundamental limitations of existing architectures, with Engram being the latest significant contribution.\n*   **Advancements in Mixture-of-Experts (MoE)**: Continued focus on refining MoE architectures for better specialization, efficiency, and scalability.\n*   **Specialized LLMs for Domain Expertise**: Development of tailored models for critical domains like coding and mathematics to address specific industry needs.\n*   **Multimodal Understanding and Generation**: Expanding capabilities to process and generate information across different modalities, particularly vision and language.\n*   **Performance and Efficiency**: A persistent effort to enhance model performance (accuracy, benchmarks) while ensuring computational and economic efficiency.\n*   **Open-Source Contributions**: A commitment to making research and models available to the community, fostering collaboration and accelerating AI development.\n\n## 5. Gap Analysis and Future Outlook\n\nWhile the Engram module is presented as a foundational element for future models, concrete details regarding the formal release and specifications of **DeepSeek V4** itself are not yet publicly detailed, with its architecture being inferred from the Engram innovation. The absence of specific new *product* releases beyond research papers, as indicated by the lack of data from the 'deepseek_llm_new_releases_T002' input, suggests that current efforts are heavily focused on foundational research and architectural development that will underpin future productization.\n\nThe successful integration of Engram, combined with DeepSeek's ongoing work in MoE and specialized models, positions the organization to continue making significant contributions to the LLM landscape. The emphasis on open-source collaboration further suggests a strategy of broad impact and community-driven advancement.\n\n## 6. Conclusion\n\nDeepSeek AI is at the forefront of LLM innovation, with the Engram module marking a pivotal advancement in addressing core architectural inefficiencies. This, coupled with a broad and impactful research portfolio spanning specialized domains, multimodality, and efficiency, indicates a strong commitment to pushing the boundaries of artificial intelligence. The company's focus on open-source contributions ensures that these advancements will likely benefit the wider AI community, paving the way for more powerful, efficient, and capable LLMs in the future.",
        "call_self": false,
        "cost": 0.0014579000000000003,
        "input_tokens": 6499,
        "output_tokens": 2020,
        "total_tokens": 8519,
        "executed_model": "gemini:gemini-2.5-flash-lite"
      },
      "error": null,
      "cost": 0.0014579000000000003,
      "start_time": "2026-01-24T06:01:45.256119",
      "end_time": "2026-01-24T06:01:54.444333",
      "execution_time": 9.188214,
      "iterations": [
        {
          "iteration": 1,
          "output": {
            "final_format": "markdown",
            "markdown_report": "# DeepSeek LLM Innovations and Research Update (January 2026)\n\nThis report details the latest advancements and research from DeepSeek in the field of Large Language Models (LLMs), focusing on architectural innovations, key research themes, and recent developments as of January 2026.\n\n## 1. Executive Summary\n\nDeepSeek AI, in collaboration with Peking University, has unveiled a significant architectural innovation with the **Engram module**, designed to address fundamental memory limitations inherent in Transformer-based LLMs. This novel conditional memory architecture promises O(1) knowledge lookup, decoupling static knowledge retrieval from intensive computation and freeing up LLM layers for more profound reasoning. This advancement is poised to be a cornerstone for future DeepSeek models, potentially powering the anticipated DeepSeek V4. Beyond Engram, DeepSeek continues its strong trajectory in developing specialized LLMs for code and mathematics, advancing multimodal capabilities, and refining Mixture-of-Experts (MoE) architectures, all while maintaining a commitment to open-source contributions.\n\n## 2. Core Innovation: The Engram Module\n\nThe Engram module represents a paradigm shift in LLM architecture, directly tackling the inefficiencies of static knowledge retrieval within the Transformer framework. Unlike traditional LLMs that simulate knowledge lookup through computationally expensive multi-layer processing, Engram provides a deterministic, constant-time (O(1)) mechanism for accessing factual information. This is achieved by decoupling knowledge storage and retrieval from the core computational layers, thereby reducing compute waste and alleviating GPU memory bottlenecks.\n\n### 2.1. Problem Addressed: Transformer's Memory Limitation\n\nModern LLMs, including DeepSeek V3, predominantly utilize the Transformer architecture, which, despite its power, lacks a native mechanism for efficient knowledge lookup. Tasks that ideally require a simple dictionary-like retrieval, such as identifying a known entity or a common phrase, are instead processed through extensive computations across multiple layers. This is fundamentally wasteful, as it consumes valuable computational resources that could otherwise be dedicated to complex reasoning and problem-solving.\n\n### 2.2. Engram's Architecture and Functionality\n\nThe Engram module introduces a new axis of sparsity, augmenting existing LLM architectures. It acts as a complementary component, leveraging lessons from classical N-gram embeddings. The core of Engram's functionality lies in its ability to store static patternslike named entities, fixed phrases, and formulasin a dedicated embedding matrix. This matrix is learned during the training process. During inference, these patterns are retrieved using computationally inexpensive lookup operations.\n\nKey features of the Engram module include:\n\n*   **O(1) Knowledge Lookup**: Enables constant-time retrieval of static knowledge.\n*   **Conditional Memory**: Memory access is conditional, guided by the model's current state.\n*   **New Axis of Sparsity**: Introduces a distinct dimension of sparsity beyond computational sparsity.\n*   **Complementary to MoE**: Works in conjunction with Mixture-of-Experts (MoE) architectures.\n*   **Tokenizer Compression**: Reduces vocabulary size by 23% by mapping equivalent tokens to canonical forms.\n*   **Multi-Head Hashing**: Employs multiple hash heads to mitigate collisions when storing N-grams.\n*   **Context-Aware Gating**: A gating mechanism uses the model's hidden state to query retrieved memory, ensuring contextual relevance.\n\n### 2.3. Benefits and Impact\n\nThe implementation of the Engram module offers a multitude of benefits:\n\n*   **Fixes Transformer's Memory Flaws**: Directly addresses the inefficiency of simulating knowledge retrieval through computation.\n*   **Reduces Compute Waste**: Minimizes unnecessary computational cycles spent on recalling static information.\n*   **Boosts Reasoning Capabilities**: Frees up LLM layers to focus on deeper, more complex reasoning tasks.\n*   **Enables LLM Scaling**: Helps overcome memory bottlenecks that limit the scalability of LLMs.\n*   **Reduces GPU Memory Bottleneck**: Optimizes memory usage, particularly crucial given the scarcity and cost of GPU High-Bandwidth Memory (HBM).\n*   **Improved Long-Context Accuracy**: Demonstrated potential for improved accuracy in handling long contexts, achieving 97% in specific tests.\n*   **Efficient Offloading**: Capable of offloading a significant number of parameters (e.g., 100 billion) to CPU/SSD with minimal (<3%) overhead.\n\nThe research paper associated with this innovation is titled \"Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models,\" and it was released on January 12, 2026. This development is a significant collaboration between DeepSeek AI and Peking University.\n\n## 3. DeepSeek's Research and Model Portfolio\n\nDeepSeek has a robust and diverse research agenda, reflected in its extensive portfolio of published papers and developed models. The following outlines key areas and notable contributions:\n\n### 3.1. Architectural Innovations & MoE Advancements\n\n*   **DeepSeek V3**: A prominent LLM powered by the Mixture of Experts (MoE) architecture.\n*   **DeepSeek Engram V4 Architecture**: The Engram module is strongly hinted to be the foundational architecture for the forthcoming DeepSeek V4, representing a significant evolution.\n*   **DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models** (Published: January 11, 2024): Explores enhancing MoE models through specialized experts.\n*   **Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models** (Published: July 2, 2024): Further research into expert specialization for sparse LLM architectures.\n*   **Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts** (Published: August 28, 2024): Focuses on improving the efficiency of MoE models through advanced load balancing.\n\n### 3.2. Specialized LLMs for Domain Expertise\n\n*   **DeepSeek-Coder**: Developed for code intelligence, with its second iteration, **DeepSeek-Coder-V2** (Published: June 17, 2024), aiming to compete with closed-source models.\n*   **DeepSeekMath**: Pushing the boundaries of mathematical reasoning in open language models (Published: February 5, 2024).\n*   **DeepSeek-Prover**: Advancing theorem proving capabilities in LLMs through large-scale synthetic data (Published: May 23, 2024), with an updated version **DeepSeek-Prover-V1.5** (Published: August 15, 2024) utilizing proof assistant feedback.\n\n### 3.3. Multimodal Understanding and Generation\n\n*   **DeepSeek-VL**: Focuses on real-world vision-language understanding (Published: March 8, 2024).\n*   **Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation** (Published: October 17, 2024) and **JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation** (Published: November 12, 2024): These research efforts aim for unified multimodal processing.\n*   **DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding** (Published: December 13, 2024): Integrates MoE into vision-language models for enhanced capabilities.\n\n### 3.4. Performance, Efficiency, and Broader Research\n\n*   **DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model** (Published: May 7, 2024): Highlights efficiency alongside performance in MoE models.\n*   **Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep Learning** (Published: August 26, 2024): Explores cost-effective solutions for AI hardware.\n*   **DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning**: A research direction focused on enhancing reasoning through RL.\n*   **DeepSeek-V3 Technical Report** (Published: December 27, 2024): Provides comprehensive details on the DeepSeek V3 model.\n\n### 3.5. Foundational Research\n\n*   **DeepSeek LLM: Scaling Open-Source Language Models with Longtermism** (Published: January 5, 2024): A foundational paper outlining strategies for scaling open-source LLMs.\n\n## 4. Key Themes in DeepSeek's Development\n\nSeveral overarching themes define DeepSeek's strategic direction in LLM research and development:\n\n*   **Architectural Innovation for LLMs**: A consistent drive to overcome fundamental limitations of existing architectures, with Engram being the latest significant contribution.\n*   **Advancements in Mixture-of-Experts (MoE)**: Continued focus on refining MoE architectures for better specialization, efficiency, and scalability.\n*   **Specialized LLMs for Domain Expertise**: Development of tailored models for critical domains like coding and mathematics to address specific industry needs.\n*   **Multimodal Understanding and Generation**: Expanding capabilities to process and generate information across different modalities, particularly vision and language.\n*   **Performance and Efficiency**: A persistent effort to enhance model performance (accuracy, benchmarks) while ensuring computational and economic efficiency.\n*   **Open-Source Contributions**: A commitment to making research and models available to the community, fostering collaboration and accelerating AI development.\n\n## 5. Gap Analysis and Future Outlook\n\nWhile the Engram module is presented as a foundational element for future models, concrete details regarding the formal release and specifications of **DeepSeek V4** itself are not yet publicly detailed, with its architecture being inferred from the Engram innovation. The absence of specific new *product* releases beyond research papers, as indicated by the lack of data from the 'deepseek_llm_new_releases_T002' input, suggests that current efforts are heavily focused on foundational research and architectural development that will underpin future productization.\n\nThe successful integration of Engram, combined with DeepSeek's ongoing work in MoE and specialized models, positions the organization to continue making significant contributions to the LLM landscape. The emphasis on open-source collaboration further suggests a strategy of broad impact and community-driven advancement.\n\n## 6. Conclusion\n\nDeepSeek AI is at the forefront of LLM innovation, with the Engram module marking a pivotal advancement in addressing core architectural inefficiencies. This, coupled with a broad and impactful research portfolio spanning specialized domains, multimodality, and efficiency, indicates a strong commitment to pushing the boundaries of artificial intelligence. The company's focus on open-source contributions ensures that these advancements will likely benefit the wider AI community, paving the way for more powerful, efficient, and capable LLMs in the future.",
            "call_self": false,
            "cost": 0.0014579000000000003,
            "input_tokens": 6499,
            "output_tokens": 2020,
            "total_tokens": 8519,
            "executed_model": "gemini:gemini-2.5-flash-lite"
          }
        }
      ],
      "input_tokens": 6499,
      "output_tokens": 2020,
      "total_tokens": 8519
    },
    {
      "id": "T005",
      "description": "Save the formatted DeepSeek LLM update to DeepSeek_Update.md.",
      "agent": "CoderAgent",
      "agent_prompt": "Save the content from 'formatted_deepseek_update_T004' into a file named 'DeepSeek_Update.md' in the DATA_DIR.",
      "reads": [
        "formatted_deepseek_update_T004"
      ],
      "writes": [
        "file_DeepSeek_Update.md_T005"
      ],
      "status": "completed",
      "output": {
        "code_variants": {
          "CODE_1A": "import os\n\nfile_content = formatted_deepseek_update_T004\nfile_path = os.path.join(DATA_DIR, 'DeepSeek_Update.md')\n\nwith open(file_path, 'w', encoding='utf-8') as f:\n    f.write(file_content)\n\nreturn { \"created_file_DeepSeek_Update.md_T005\": file_path }"
        },
        "cost": 2.71e-05,
        "input_tokens": 159,
        "output_tokens": 28,
        "total_tokens": 187,
        "executed_model": "gemini:gemini-2.5-flash-lite",
        "execution_result": null,
        "execution_status": "error",
        "execution_error": "TypeError: 'str' object cannot be interpreted as an integer",
        "execution_time": "2026-01-24 11:31:55",
        "executed_variant": "CODE_1A",
        "execution_logs": null
      },
      "error": null,
      "cost": 2.71e-05,
      "start_time": "2026-01-24T06:01:54.454838",
      "end_time": "2026-01-24T06:01:55.795796",
      "execution_time": 1.340958,
      "iterations": [
        {
          "iteration": 1,
          "output": {
            "code_variants": {
              "CODE_1A": "import os\n\nfile_content = formatted_deepseek_update_T004\nfile_path = os.path.join(DATA_DIR, 'DeepSeek_Update.md')\n\nwith open(file_path, 'w', encoding='utf-8') as f:\n    f.write(file_content)\n\nreturn { \"created_file_DeepSeek_Update.md_T005\": file_path }"
            },
            "cost": 2.71e-05,
            "input_tokens": 159,
            "output_tokens": 28,
            "total_tokens": 187,
            "executed_model": "gemini:gemini-2.5-flash-lite"
          },
          "execution_result": {
            "status": "error",
            "error": "TypeError: 'str' object cannot be interpreted as an integer",
            "traceback": "Traceback (most recent call last):\n  File \"/Users/rohanshravan/TSAI/Arcturus/tools/sandbox.py\", line 365, in run_user_code\n    returned = await asyncio.wait_for(local_vars[\"__main\"](), timeout=timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/rohanshravan/.pyenv/versions/3.11.5/lib/python3.11/asyncio/tasks.py\", line 479, in wait_for\n    return fut.result()\n           ^^^^^^^^^^^^\n  File \"<user_code>\", line 13, in __main\nTypeError: 'str' object cannot be interpreted as an integer\n",
            "execution_time": "2026-01-24 11:31:55",
            "total_time": "0.033",
            "executed_variant": "CODE_1A"
          }
        }
      ],
      "input_tokens": 159,
      "output_tokens": 28,
      "total_tokens": 187
    }
  ],
  "edges": [
    {
      "source": "ROOT",
      "target": "Query"
    },
    {
      "source": "Query",
      "target": "T001"
    },
    {
      "source": "Query",
      "target": "T002"
    },
    {
      "source": "T001",
      "target": "T003"
    },
    {
      "source": "T002",
      "target": "T003"
    },
    {
      "source": "T003",
      "target": "T004"
    },
    {
      "source": "T004",
      "target": "T005"
    }
  ]
}